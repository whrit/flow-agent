name: ðŸŽ¯ Truth Scoring Pipeline

on:
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened]
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      target_branch:
        description: 'Target branch for comparison'
        required: false
        default: 'main'
      scoring_depth:
        description: 'Scoring depth level'
        required: false
        default: 'standard'
        type: choice
        options:
          - quick
          - standard
          - comprehensive

env:
  NODE_VERSION: '20'
  TRUTH_SCORE_THRESHOLD: 85
  REGRESSION_THRESHOLD: 10

jobs:
  # Initialize truth scoring session
  truth-scoring-setup:
    name: ðŸš€ Truth Scoring Setup
    runs-on: ubuntu-latest
    outputs:
      scoring-session-id: ${{ steps.setup.outputs.scoring-session-id }}
      baseline-ref: ${{ steps.setup.outputs.baseline-ref }}
      comparison-strategy: ${{ steps.setup.outputs.comparison-strategy }}
    steps:
      - name: Checkout PR code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup scoring session
        id: setup
        run: |
          SCORING_SESSION="truth-$(date +%Y%m%d-%H%M%S)-${{ github.sha }}"
          echo "scoring-session-id=$SCORING_SESSION" >> $GITHUB_OUTPUT
          
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "baseline-ref=${{ github.event.pull_request.base.sha }}" >> $GITHUB_OUTPUT
            echo "comparison-strategy=diff" >> $GITHUB_OUTPUT
          else
            echo "baseline-ref=HEAD~1" >> $GITHUB_OUTPUT
            echo "comparison-strategy=commit" >> $GITHUB_OUTPUT
          fi

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Initialize truth scoring database
        run: |
          mkdir -p truth-scoring-data
          cat > truth-scoring-data/session-config.json << 'EOF'
          {
            "sessionId": "${{ steps.setup.outputs.scoring-session-id }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "baselineRef": "${{ steps.setup.outputs.baseline-ref }}",
            "currentRef": "${{ github.sha }}",
            "strategy": "${{ steps.setup.outputs.comparison-strategy }}",
            "thresholds": {
              "truthScore": ${{ env.TRUTH_SCORE_THRESHOLD }},
              "regression": ${{ env.REGRESSION_THRESHOLD }}
            }
          }
          EOF

      - name: Upload session config
        uses: actions/upload-artifact@v4
        with:
          name: truth-scoring-config-${{ steps.setup.outputs.scoring-session-id }}
          path: truth-scoring-data/
          retention-days: 30

  # Code accuracy scoring
  code-accuracy-scoring:
    name: ðŸ“ Code Accuracy Scoring
    runs-on: ubuntu-latest
    needs: truth-scoring-setup
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Download session config
        uses: actions/download-artifact@v4
        with:
          name: truth-scoring-config-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}
          path: truth-scoring-data/

      - name: Analyze code changes
        run: |
          echo "ðŸ” Analyzing code changes for accuracy..."
          
          # Get changed files
          git diff --name-only ${{ needs.truth-scoring-setup.outputs.baseline-ref }}..HEAD > changed-files.txt
          
          # TypeScript/JavaScript specific analysis
          grep -E '\.(ts|js|tsx|jsx)$' changed-files.txt > code-files.txt || touch code-files.txt
          
          # Count lines changed
          TOTAL_CHANGES=$(git diff --shortstat ${{ needs.truth-scoring-setup.outputs.baseline-ref }}..HEAD | grep -o '[0-9]* insertions' | grep -o '[0-9]*' || echo "0")
          TOTAL_DELETIONS=$(git diff --shortstat ${{ needs.truth-scoring-setup.outputs.baseline-ref }}..HEAD | grep -o '[0-9]* deletions' | grep -o '[0-9]*' || echo "0")
          
          echo "ðŸ“Š Code change metrics:"
          echo "  - Files changed: $(wc -l < changed-files.txt)"
          echo "  - Code files: $(wc -l < code-files.txt)"
          echo "  - Lines added: $TOTAL_CHANGES"
          echo "  - Lines deleted: $TOTAL_DELETIONS"

      - name: Run static analysis scoring
        run: |
          echo "ðŸ” Running static analysis for truth scoring..."
          
          # ESLint analysis with custom rules for truth scoring
          npm run lint -- --format json --output-file truth-scoring-data/eslint-analysis.json || true
          
          # TypeScript compilation check
          npm run typecheck > truth-scoring-data/typescript-check.log 2>&1 || true
          
          # Complexity analysis
          npx complexity-report --format json --output truth-scoring-data/complexity-analysis.json src/ || true

      - name: Calculate code accuracy score
        run: |
          echo "ðŸ“Š Calculating code accuracy score..."
          
          node -e "
          const fs = require('fs');
          
          let score = 100;
          let details = [];
          
          // ESLint penalty calculation
          try {
            const eslintData = JSON.parse(fs.readFileSync('truth-scoring-data/eslint-analysis.json', 'utf8'));
            const totalErrors = eslintData.reduce((sum, file) => sum + file.errorCount, 0);
            const totalWarnings = eslintData.reduce((sum, file) => sum + file.warningCount, 0);
            
            const eslintPenalty = (totalErrors * 2) + (totalWarnings * 0.5);
            score -= Math.min(eslintPenalty, 20);
            details.push(\`ESLint: -\${eslintPenalty.toFixed(1)} (errors: \${totalErrors}, warnings: \${totalWarnings})\`);
          } catch (e) {
            details.push('ESLint: Analysis failed');
          }
          
          // TypeScript penalty
          try {
            const tsLog = fs.readFileSync('truth-scoring-data/typescript-check.log', 'utf8');
            const tsErrors = (tsLog.match(/error TS/g) || []).length;
            const tsPenalty = tsErrors * 3;
            score -= Math.min(tsPenalty, 15);
            details.push(\`TypeScript: -\${tsPenalty.toFixed(1)} (errors: \${tsErrors})\`);
          } catch (e) {
            details.push('TypeScript: Check failed');
          }
          
          const result = {
            score: Math.max(score, 0),
            details: details,
            timestamp: new Date().toISOString()
          };
          
          fs.writeFileSync('truth-scoring-data/code-accuracy-score.json', JSON.stringify(result, null, 2));
          console.log(\`Code Accuracy Score: \${result.score.toFixed(1)}/100\`);
          console.log('Details:', result.details.join(', '));
          "

      - name: Upload code accuracy results
        uses: actions/upload-artifact@v4
        with:
          name: code-accuracy-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}
          path: truth-scoring-data/
          retention-days: 30

  # Test coverage truth scoring
  test-coverage-scoring:
    name: ðŸ§ª Test Coverage Scoring
    runs-on: ubuntu-latest
    needs: truth-scoring-setup
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Download session config
        uses: actions/download-artifact@v4
        with:
          name: truth-scoring-config-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}
          path: truth-scoring-data/

      - name: Run test coverage analysis
        run: |
          echo "ðŸ§ª Running test coverage analysis..."
          
          # Run tests with coverage
          npm run test:coverage || true
          
          # Extract coverage metrics
          if [ -f "coverage/coverage-summary.json" ]; then
            cp coverage/coverage-summary.json truth-scoring-data/
          fi

      - name: Calculate test coverage score
        run: |
          echo "ðŸ“Š Calculating test coverage score..."
          
          node -e "
          const fs = require('fs');
          
          let score = 0;
          let details = [];
          
          try {
            const coverage = JSON.parse(fs.readFileSync('truth-scoring-data/coverage-summary.json', 'utf8'));
            const total = coverage.total;
            
            // Coverage scoring weights
            const lineScore = total.lines.pct * 0.4;
            const branchScore = total.branches.pct * 0.3;
            const functionScore = total.functions.pct * 0.2;
            const statementScore = total.statements.pct * 0.1;
            
            score = lineScore + branchScore + functionScore + statementScore;
            
            details.push(\`Lines: \${total.lines.pct}%\`);
            details.push(\`Branches: \${total.branches.pct}%\`);
            details.push(\`Functions: \${total.functions.pct}%\`);
            details.push(\`Statements: \${total.statements.pct}%\`);
            
          } catch (e) {
            score = 0;
            details.push('Coverage data not available');
          }
          
          const result = {
            score: Math.round(score * 100) / 100,
            details: details,
            timestamp: new Date().toISOString()
          };
          
          fs.writeFileSync('truth-scoring-data/test-coverage-score.json', JSON.stringify(result, null, 2));
          console.log(\`Test Coverage Score: \${result.score.toFixed(1)}/100\`);
          console.log('Details:', result.details.join(', '));
          "

      - name: Upload test coverage results
        uses: actions/upload-artifact@v4
        with:
          name: test-coverage-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}
          path: truth-scoring-data/
          retention-days: 30

  # Performance regression scoring
  performance-regression-scoring:
    name: âš¡ Performance Regression Scoring
    runs-on: ubuntu-latest
    needs: truth-scoring-setup
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Download session config
        uses: actions/download-artifact@v4
        with:
          name: truth-scoring-config-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}
          path: truth-scoring-data/

      - name: Run baseline performance tests
        run: |
          echo "âš¡ Running baseline performance tests..."
          
          git checkout ${{ needs.truth-scoring-setup.outputs.baseline-ref }}
          npm ci
          
          # Run performance benchmarks for baseline
          timeout 300s npm run test:performance > truth-scoring-data/baseline-performance.log 2>&1 || true
          
          git checkout ${{ github.sha }}
          npm ci

      - name: Run current performance tests
        run: |
          echo "âš¡ Running current performance tests..."
          
          # Run performance benchmarks for current code
          timeout 300s npm run test:performance > truth-scoring-data/current-performance.log 2>&1 || true

      - name: Calculate performance score
        run: |
          echo "ðŸ“Š Calculating performance regression score..."
          
          node -e "
          const fs = require('fs');
          
          let score = 100;
          let details = [];
          
          try {
            const baselineLog = fs.readFileSync('truth-scoring-data/baseline-performance.log', 'utf8');
            const currentLog = fs.readFileSync('truth-scoring-data/current-performance.log', 'utf8');
            
            // Simple performance comparison (could be enhanced with actual metrics)
            const baselineTime = (baselineLog.match(/(\d+)ms/g) || []).map(t => parseInt(t));
            const currentTime = (currentLog.match(/(\d+)ms/g) || []).map(t => parseInt(t));
            
            if (baselineTime.length > 0 && currentTime.length > 0) {
              const avgBaseline = baselineTime.reduce((a, b) => a + b, 0) / baselineTime.length;
              const avgCurrent = currentTime.reduce((a, b) => a + b, 0) / currentTime.length;
              
              const performanceChange = ((avgCurrent - avgBaseline) / avgBaseline) * 100;
              
              if (performanceChange > 0) {
                // Performance regression penalty
                const penalty = Math.min(performanceChange * 2, 50);
                score -= penalty;
                details.push(\`Regression: +\${performanceChange.toFixed(1)}% (-\${penalty.toFixed(1)})\`);
              } else {
                // Performance improvement bonus
                const bonus = Math.min(Math.abs(performanceChange), 10);
                score = Math.min(score + bonus, 100);
                details.push(\`Improvement: \${performanceChange.toFixed(1)}% (+\${bonus.toFixed(1)})\`);
              }
            } else {
              details.push('Performance comparison inconclusive');
            }
            
          } catch (e) {
            details.push('Performance test failed');
          }
          
          const result = {
            score: Math.max(score, 0),
            details: details,
            timestamp: new Date().toISOString()
          };
          
          fs.writeFileSync('truth-scoring-data/performance-score.json', JSON.stringify(result, null, 2));
          console.log(\`Performance Score: \${result.score.toFixed(1)}/100\`);
          console.log('Details:', result.details.join(', '));
          "

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        with:
          name: performance-scoring-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}
          path: truth-scoring-data/
          retention-days: 30

  # Documentation truth scoring
  documentation-scoring:
    name: ðŸ“š Documentation Scoring
    runs-on: ubuntu-latest
    needs: truth-scoring-setup
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Download session config
        uses: actions/download-artifact@v4
        with:
          name: truth-scoring-config-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}
          path: truth-scoring-data/

      - name: Analyze documentation changes
        run: |
          echo "ðŸ“š Analyzing documentation changes..."
          
          # Get documentation files
          git diff --name-only ${{ needs.truth-scoring-setup.outputs.baseline-ref }}..HEAD | grep -E '\.(md|txt|rst)$' > doc-files.txt || touch doc-files.txt
          
          # Check for README updates
          README_CHANGED=$(git diff --name-only ${{ needs.truth-scoring-setup.outputs.baseline-ref }}..HEAD | grep -c README.md || echo "0")
          
          # Check for CHANGELOG updates
          CHANGELOG_CHANGED=$(git diff --name-only ${{ needs.truth-scoring-setup.outputs.baseline-ref }}..HEAD | grep -c CHANGELOG.md || echo "0")
          
          echo "ðŸ“Š Documentation metrics:"
          echo "  - Doc files changed: $(wc -l < doc-files.txt)"
          echo "  - README updated: $README_CHANGED"
          echo "  - CHANGELOG updated: $CHANGELOG_CHANGED"

      - name: Calculate documentation score
        run: |
          echo "ðŸ“Š Calculating documentation score..."
          
          node -e "
          const fs = require('fs');
          
          let score = 70; // Base score
          let details = [];
          
          try {
            // Check if documentation files exist
            if (fs.existsSync('README.md')) {
              score += 10;
              details.push('README.md exists (+10)');
            }
            
            if (fs.existsSync('CHANGELOG.md')) {
              score += 10;
              details.push('CHANGELOG.md exists (+10)');
            }
            
            if (fs.existsSync('LICENSE')) {
              score += 5;
              details.push('LICENSE exists (+5)');
            }
            
            // Check documentation updates for significant changes
            const docFiles = fs.readFileSync('doc-files.txt', 'utf8').trim();
            if (docFiles) {
              const fileCount = docFiles.split('\n').filter(f => f.trim()).length;
              const bonus = Math.min(fileCount * 2, 10);
              score += bonus;
              details.push(\`Doc files updated: \${fileCount} (+\${bonus})\`);
            }
            
          } catch (e) {
            details.push('Documentation analysis failed');
          }
          
          const result = {
            score: Math.min(score, 100),
            details: details,
            timestamp: new Date().toISOString()
          };
          
          fs.writeFileSync('truth-scoring-data/documentation-score.json', JSON.stringify(result, null, 2));
          console.log(\`Documentation Score: \${result.score.toFixed(1)}/100\`);
          console.log('Details:', result.details.join(', '));
          "

      - name: Upload documentation results
        uses: actions/upload-artifact@v4
        with:
          name: documentation-scoring-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}
          path: truth-scoring-data/
          retention-days: 30

  # Final truth score calculation
  truth-score-calculation:
    name: ðŸŽ¯ Truth Score Calculation
    runs-on: ubuntu-latest
    needs: [
      truth-scoring-setup,
      code-accuracy-scoring,
      test-coverage-scoring,
      performance-regression-scoring,
      documentation-scoring
    ]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all scoring artifacts
        uses: actions/download-artifact@v4
        with:
          path: truth-scoring-results/

      - name: Calculate final truth score
        id: final-score
        run: |
          echo "ðŸŽ¯ Calculating final truth score..."
          
          mkdir -p final-results
          
          node -e "
          const fs = require('fs');
          const path = require('path');
          
          // Score weights
          const weights = {
            codeAccuracy: 0.35,
            testCoverage: 0.25,
            performance: 0.25,
            documentation: 0.15
          };
          
          let scores = {};
          let totalScore = 0;
          let details = [];
          
          try {
            // Load individual scores
            const codeAccuracy = JSON.parse(fs.readFileSync('truth-scoring-results/code-accuracy-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}/code-accuracy-score.json', 'utf8'));
            const testCoverage = JSON.parse(fs.readFileSync('truth-scoring-results/test-coverage-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}/test-coverage-score.json', 'utf8'));
            const performance = JSON.parse(fs.readFileSync('truth-scoring-results/performance-scoring-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}/performance-score.json', 'utf8'));
            const documentation = JSON.parse(fs.readFileSync('truth-scoring-results/documentation-scoring-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}/documentation-score.json', 'utf8'));
            
            scores = {
              codeAccuracy: codeAccuracy.score,
              testCoverage: testCoverage.score,
              performance: performance.score,
              documentation: documentation.score
            };
            
            // Calculate weighted total
            totalScore = (
              scores.codeAccuracy * weights.codeAccuracy +
              scores.testCoverage * weights.testCoverage +
              scores.performance * weights.performance +
              scores.documentation * weights.documentation
            );
            
            details = [
              \`Code Quality: \${scores.codeAccuracy.toFixed(1)} (weight: \${(weights.codeAccuracy * 100)}%)\`,
              \`Test Coverage: \${scores.testCoverage.toFixed(1)} (weight: \${(weights.testCoverage * 100)}%)\`,
              \`Performance: \${scores.performance.toFixed(1)} (weight: \${(weights.performance * 100)}%)\`,
              \`Documentation: \${scores.documentation.toFixed(1)} (weight: \${(weights.documentation * 100)}%)\`
            ];
            
          } catch (e) {
            console.error('Error calculating final score:', e.message);
            totalScore = 0;
            details = ['Score calculation failed'];
          }
          
          const result = {
            sessionId: '${{ needs.truth-scoring-setup.outputs.scoring-session-id }}',
            finalScore: Math.round(totalScore * 100) / 100,
            individualScores: scores,
            weights: weights,
            details: details,
            timestamp: new Date().toISOString(),
            passed: totalScore >= ${{ env.TRUTH_SCORE_THRESHOLD }},
            threshold: ${{ env.TRUTH_SCORE_THRESHOLD }}
          };
          
          fs.writeFileSync('final-results/truth-score-final.json', JSON.stringify(result, null, 2));
          
          console.log(\`\\nðŸŽ¯ FINAL TRUTH SCORE: \${result.finalScore.toFixed(1)}/100\`);
          console.log(\`Threshold: \${result.threshold}\`);
          console.log(\`Status: \${result.passed ? 'âœ… PASSED' : 'âŒ FAILED'}\`);
          console.log('\\nBreakdown:');
          result.details.forEach(detail => console.log(\`  - \${detail}\`));
          
          // Set outputs for GitHub Actions
          console.log(\`final-score=\${result.finalScore.toFixed(1)}\`);
          console.log(\`passed=\${result.passed}\`);
          " | tee final-score-output.txt
          
          # Extract outputs
          FINAL_SCORE=$(grep "final-score=" final-score-output.txt | cut -d'=' -f2)
          PASSED=$(grep "passed=" final-score-output.txt | cut -d'=' -f2)
          
          echo "final-score=$FINAL_SCORE" >> $GITHUB_OUTPUT
          echo "passed=$PASSED" >> $GITHUB_OUTPUT

      - name: Generate truth score report
        run: |
          echo "ðŸ“Š Generating truth score report..."
          
          cat > final-results/truth-score-report.md << 'EOF'
          # ðŸŽ¯ Truth Score Report
          
          **Session ID:** ${{ needs.truth-scoring-setup.outputs.scoring-session-id }}
          **Final Score:** ${{ steps.final-score.outputs.final-score }}/100
          **Status:** ${{ steps.final-score.outputs.passed == 'true' && 'âœ… PASSED' || 'âŒ FAILED' }}
          **Threshold:** ${{ env.TRUTH_SCORE_THRESHOLD }}
          
          ## Score Breakdown
          
          | Component | Score | Weight | Contribution |
          |-----------|-------|--------|--------------|
          | Code Quality | - | 35% | - |
          | Test Coverage | - | 25% | - |
          | Performance | - | 25% | - |
          | Documentation | - | 15% | - |
          
          ## Recommendations
          
          ${{ steps.final-score.outputs.passed == 'true' && 'âœ… **Great work!** Your changes meet the truth scoring threshold.' || 'âŒ **Improvements needed** to meet the truth scoring threshold.' }}
          
          ### Next Steps
          
          1. Review individual component scores
          2. Focus on lowest-scoring areas
          3. Re-run pipeline after improvements
          
          ---
          *Generated by Truth Scoring Pipeline*
          EOF

      - name: Upload final truth score
        uses: actions/upload-artifact@v4
        with:
          name: truth-score-final-${{ needs.truth-scoring-setup.outputs.scoring-session-id }}
          path: final-results/
          retention-days: 90

      - name: Comment PR with truth score
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('final-results/truth-score-report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

      - name: Fail if truth score below threshold
        if: steps.final-score.outputs.passed == 'false'
        run: |
          echo "âŒ Truth score ${{ steps.final-score.outputs.final-score }} is below threshold ${{ env.TRUTH_SCORE_THRESHOLD }}"
          exit 1