# Claude Flow Benchmark System v2.0

A production-ready benchmarking system for Claude Flow that executes **real commands** and measures **actual performance metrics**.

## ğŸš€ Quick Start

```bash
# Install dependencies
cd benchmark
pip install -e .

# Run real benchmark
python examples/real_swarm_benchmark.py "Build REST API"

# Run comprehensive suite
python run_real_benchmarks.py --mode comprehensive
```

## âœ¨ Key Features

- **Real Command Execution**: Executes actual `./claude-flow` commands via subprocess
- **Stream JSON Parsing**: Real-time parsing of `--non-interactive --output-format stream-json`
- **Authentic Metrics**: Token usage, execution time, and resource consumption from real runs
- **No Simulations**: 100% real execution, no mocks or placeholders
- **MLE-STAR Integration**: Ensemble learning benchmarks
- **CLAUDE.md Optimizer**: Generate optimized configurations for specific use cases

## ğŸ“Š Supported Commands

### Swarm Benchmarking
```bash
./claude-flow swarm "objective" --non-interactive --output-format stream-json
```

### Hive-Mind Benchmarking
```bash
./claude-flow hive-mind spawn "task" --non-interactive
```

### SPARC Mode Benchmarking
```bash
./claude-flow sparc run code "task" --non-interactive
```

## ğŸ—ï¸ Architecture

```
benchmark/
â”œâ”€â”€ src/swarm_benchmark/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ claude_flow_real_executor.py  # Real command executor
â”‚   â”‚   â”œâ”€â”€ real_benchmark_engine_v2.py   # Production benchmark engine
â”‚   â”‚   â””â”€â”€ benchmark_engine.py           # Core engine with real support
â”‚   â”œâ”€â”€ scenarios/
â”‚   â”‚   â””â”€â”€ real_benchmarks.py            # Real benchmark scenarios
â”‚   â”œâ”€â”€ mle_star/                         # MLE-STAR ensemble integration
â”‚   â”œâ”€â”€ claude_optimizer/                 # CLAUDE.md optimization
â”‚   â”œâ”€â”€ automation/                       # Batch processing & pipelines
â”‚   â””â”€â”€ advanced_metrics/                 # Token & performance metrics
â”œâ”€â”€ examples/                              # Working examples
â”œâ”€â”€ tests/                                 # Comprehensive test suite
â””â”€â”€ tools/                                 # Utility scripts
```

## ğŸ“ˆ Real Metrics Captured

- **Token Usage**: Input/output tokens from Claude API
- **Execution Time**: Real command runtime
- **Agent Activity**: Spawned agents, completed tasks
- **Resource Usage**: CPU, memory, disk I/O
- **Error Rates**: Command failures and recovery
- **Consensus Metrics**: Hive-mind decision quality

## ğŸ§ª Testing

```bash
# Quick validation
python test_real_benchmarks.py --quick

# Integration tests
pytest tests/integration/test_real_claude_flow_integration.py

# Verify real integration
python examples/verify_real_integration.py
```

## ğŸ“š Documentation

- [API Reference](docs/api_reference.md)
- [CLAUDE.md Optimizer Guide](docs/claude_optimizer_guide.md)
- [Real Benchmarks Guide](REAL_BENCHMARKS_README.md)
- [Architecture Overview](docs/real-benchmark-architecture.md)

## ğŸ¯ Example Usage

```python
from swarm_benchmark import BenchmarkEngine

# Create engine with real executor
engine = BenchmarkEngine(use_real_executor=True)

# Run real benchmark
result = await engine.run_real_benchmark(
    objective="Build microservices architecture",
    strategy="development",
    mode="distributed",
    max_agents=5
)

# Access real metrics
print(f"Tokens used: {result.metrics['total_tokens']}")
print(f"Execution time: {result.metrics['duration_seconds']}s")
print(f"Agents spawned: {result.metrics['agents_spawned']}")
```

## ğŸ”§ Configuration

The system uses real Claude Flow commands with these flags:
- `--non-interactive`: Automation mode
- `--output-format stream-json`: Structured output
- `--dangerously-skip-permissions`: Skip prompts
- `--verbose`: Detailed logging

## ğŸ“Š Performance

Validated with Claude Flow v2.0.0-alpha.87:
- Real token usage tracking
- Actual execution timing
- Live resource monitoring
- Genuine error rates

## ğŸ¤ Contributing

This is a production system designed for real benchmarking. All contributions must:
1. Use real Claude Flow commands
2. Parse actual responses
3. Measure genuine metrics
4. Include comprehensive tests

## ğŸ“„ License

MIT License - See LICENSE file for details

---

**Version**: 2.0.0  
**Status**: Production Ready  
**Real Execution**: Yes  
**Simulations**: None