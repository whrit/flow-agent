# CLAUDE.md - Optimized for Ml Pipeline

<!-- Generated by CLAUDE.md Optimizer at 2025-08-06 01:57 -->

## ðŸŽ¯ Configuration Overview
- **Use Case**: Ml Pipeline
- **Optimization Target**: balanced
- **Team Size**: N/A
- **Complexity**: medium

## âš¡ Performance Configuration
- **Execution Mode**: Batch-optimized
- **Agent Count**: 7 (ml-developer, performance-benchmarker, researcher, reviewer, tester)
- **Topology**: Mesh
- **Batch Operations**: RECOMMENDED
- **Parallel Execution**: STANDARD
- **Optimizations**:
  - Gpu Optimization
  - Batch Processing
  - Distributed Training
  - Memory Mapping

## ðŸŽ¯ Primary Objectives
1. Model training
2. Data preprocessing
3. Feature engineering
4. Implement ensemble learning with MLE-STAR
5. Achieve >90% model accuracy
6. Optimize training time <30 minutes

## ðŸš€ Execution Rules

### CRITICAL: Concurrency Requirements
```javascript
// ALWAYS execute in single message:
[
  Task("Ml Developer", "...", "ml-developer"),
  Task("Performance Benchmarker", "...", "performance-benchmarker"),
  Task("Researcher", "...", "researcher"),
  Task("Reviewer", "...", "reviewer"),
  TodoWrite({ todos: [ALL_TASKS] }),
  // ... all related operations
]```

### Critical Rules
- Use MLE-STAR for ensemble coordination
- Implement cross-validation
- Track all experiments
- Optimize hyperparameters
- ALWAYS execute models in parallel
- Batch data processing operations
- Enable memory persistence for training state
- Always run tests after changes
- Use type checking and linting
- Implement comprehensive error handling
- Request code review from reviewer agent
- Validate all inputs and outputs
- Use static analysis tools
- Implement integration testing
- Add comprehensive logging
- Verify requirements before implementation
- Use defensive programming practices
- Implement comprehensive logging
- Use distributed tracing
- Enable advanced monitoring

### Tool Priority Order
1. MultiEdit â†’ Edit â†’ Bash â†’ Read

### Ml Pipeline Workflow
**Data Preprocessing**: Batch data validation and cleaning
**Feature Engineering**: Parallel feature extraction
**Model Training**: MLE-STAR ensemble coordination
**Evaluation**: Cross-validation with multiple metrics
**Deployment**: Automated model versioning and deployment

## ðŸ¤– Agent Configuration
- **Swarm Topology**: Mesh
- **Maximum Agents**: 7
- **Preferred Agent Types**: ml-developer, performance-benchmarker, researcher, reviewer, tester

### Coordination Features
- Neural Patterns: Enabled
- Memory Sharing: Enabled
- Collective Intelligence: Enabled

## ðŸ“Š Success Metrics
- Model Accuracy: >90%
- Training Time: <30 min
- Memory Usage: <8GB
- Ensemble Consensus: >95%

## ðŸ§  MLE-STAR Configuration
- **Ensemble Size**: 5 models
- **Voting Strategy**: Weighted consensus  
- **Model Diversity**: High

### Ensemble Coordination Pattern
```python
# Initialize MLE-STAR ensemble
mcp__claude-flow__swarm_init({
  topology: "mesh",
  maxAgents: 8,
  mle_star: {
    enabled: true,
    models: ["model1", "model2", "model3", "model4", "model5"],
    consensus: "weighted_voting"
  }
})

# Spawn ML agents in parallel
[
  Task("Data Preprocessing", "...", "ml-developer"),
  Task("Feature Engineering", "...", "ml-developer"), 
  Task("Model Training", "...", "ml-developer"),
  Task("Hyperparameter Tuning", "...", "optimizer"),
  Task("Model Evaluation", "...", "tester")
]
```

## ðŸ”„ Agent Coordination Protocol