{"version":3,"sources":["../../../../src/services/agentic-flow-hooks/llm-hooks.ts"],"sourcesContent":["/**\n * LLM-specific hooks for agentic-flow integration\n * \n * Provides pre/post operation hooks for all LLM calls with\n * memory persistence and performance optimization.\n */\n\nimport { agenticHookManager } from './hook-manager.js';\nimport type {\n  AgenticHookContext,\n  HookHandlerResult,\n  LLMHookPayload,\n  LLMMetrics,\n  Pattern,\n  SideEffect,\n} from './types.js';\n\n// ===== Pre-LLM Call Hook =====\n\nexport const preLLMCallHook = {\n  id: 'agentic-pre-llm-call',\n  type: 'pre-llm-call' as const,\n  priority: 100,\n  handler: async (\n    payload: LLMHookPayload,\n    context: AgenticHookContext\n  ): Promise<HookHandlerResult> => {\n    const { provider, model, operation, request } = payload;\n    \n    // Check memory for similar requests\n    const cacheKey = generateCacheKey(provider, model, request);\n    const cached = await checkMemoryCache(cacheKey, context);\n    \n    if (cached) {\n      return {\n        continue: false, // Skip LLM call\n        modified: true,\n        payload: {\n          ...payload,\n          response: cached.response,\n          metrics: {\n            ...cached.metrics,\n            cacheHit: true,\n          },\n        },\n        sideEffects: [\n          {\n            type: 'metric',\n            action: 'increment',\n            data: { name: 'llm.cache.hits' },\n          },\n        ],\n      };\n    }\n    \n    // Load provider-specific optimizations\n    const optimizations = await loadProviderOptimizations(provider, context);\n    \n    // Apply request optimizations\n    const optimizedRequest = applyRequestOptimizations(\n      request,\n      optimizations,\n      context\n    );\n    \n    // Track pre-call metrics\n    const sideEffects: SideEffect[] = [\n      {\n        type: 'metric',\n        action: 'increment',\n        data: { name: `llm.calls.${provider}.${model}` },\n      },\n      {\n        type: 'memory',\n        action: 'store',\n        data: {\n          key: `llm:request:${context.correlationId}`,\n          value: {\n            provider,\n            model,\n            operation,\n            request: optimizedRequest,\n            timestamp: Date.now(),\n          },\n          ttl: 3600, // 1 hour\n        },\n      },\n    ];\n    \n    return {\n      continue: true,\n      modified: true,\n      payload: {\n        ...payload,\n        request: optimizedRequest,\n      },\n      sideEffects,\n    };\n  },\n};\n\n// ===== Post-LLM Call Hook =====\n\nexport const postLLMCallHook = {\n  id: 'agentic-post-llm-call',\n  type: 'post-llm-call' as const,\n  priority: 100,\n  handler: async (\n    payload: LLMHookPayload,\n    context: AgenticHookContext\n  ): Promise<HookHandlerResult> => {\n    const { provider, model, request, response, metrics } = payload;\n    \n    if (!response || !metrics) {\n      return { continue: true };\n    }\n    \n    const sideEffects: SideEffect[] = [];\n    \n    // Store response in memory for caching\n    const cacheKey = generateCacheKey(provider, model, request);\n    sideEffects.push({\n      type: 'memory',\n      action: 'store',\n      data: {\n        key: `llm:cache:${cacheKey}`,\n        value: {\n          response,\n          metrics,\n          timestamp: Date.now(),\n        },\n        ttl: determineCacheTTL(operation, response),\n      },\n    });\n    \n    // Extract patterns for neural training\n    const patterns = extractResponsePatterns(request, response, metrics);\n    if (patterns.length > 0) {\n      sideEffects.push({\n        type: 'neural',\n        action: 'train',\n        data: {\n          patterns,\n          modelId: `llm-optimizer-${provider}`,\n        },\n      });\n    }\n    \n    // Update performance metrics\n    sideEffects.push(\n      {\n        type: 'metric',\n        action: 'update',\n        data: {\n          name: `llm.latency.${provider}.${model}`,\n          value: metrics.latency,\n        },\n      },\n      {\n        type: 'metric',\n        action: 'update',\n        data: {\n          name: `llm.tokens.${provider}.${model}`,\n          value: response.usage.totalTokens,\n        },\n      },\n      {\n        type: 'metric',\n        action: 'update',\n        data: {\n          name: `llm.cost.${provider}.${model}`,\n          value: metrics.costEstimate,\n        },\n      }\n    );\n    \n    // Check for performance issues\n    if (metrics.latency > getLatencyThreshold(provider, model)) {\n      sideEffects.push({\n        type: 'notification',\n        action: 'send',\n        data: {\n          level: 'warning',\n          message: `High latency detected for ${provider}/${model}: ${metrics.latency}ms`,\n        },\n      });\n    }\n    \n    // Store provider health score\n    await updateProviderHealth(provider, metrics.providerHealth, context);\n    \n    return {\n      continue: true,\n      sideEffects,\n    };\n  },\n};\n\n// ===== LLM Error Hook =====\n\nexport const llmErrorHook = {\n  id: 'agentic-llm-error',\n  type: 'llm-error' as const,\n  priority: 100,\n  handler: async (\n    payload: LLMHookPayload,\n    context: AgenticHookContext\n  ): Promise<HookHandlerResult> => {\n    const { provider, model, error } = payload;\n    \n    if (!error) {\n      return { continue: true };\n    }\n    \n    const sideEffects: SideEffect[] = [];\n    \n    // Log error details\n    sideEffects.push({\n      type: 'log',\n      action: 'write',\n      data: {\n        level: 'error',\n        message: `LLM error from ${provider}/${model}`,\n        data: {\n          error: error.message,\n          stack: error.stack,\n          request: payload.request,\n        },\n      },\n    });\n    \n    // Update error metrics\n    sideEffects.push({\n      type: 'metric',\n      action: 'increment',\n      data: { name: `llm.errors.${provider}.${model}` },\n    });\n    \n    // Check if we should fallback\n    const fallbackProvider = await selectFallbackProvider(\n      provider,\n      model,\n      error,\n      context\n    );\n    \n    if (fallbackProvider) {\n      return {\n        continue: false, // Don't propagate error\n        modified: true,\n        payload: {\n          ...payload,\n          provider: fallbackProvider.provider,\n          model: fallbackProvider.model,\n          error: undefined, // Clear error for retry\n        },\n        sideEffects: [\n          ...sideEffects,\n          {\n            type: 'notification',\n            action: 'send',\n            data: {\n              level: 'info',\n              message: `Falling back from ${provider}/${model} to ${fallbackProvider.provider}/${fallbackProvider.model}`,\n            },\n          },\n        ],\n      };\n    }\n    \n    return {\n      continue: true,\n      sideEffects,\n    };\n  },\n};\n\n// ===== LLM Retry Hook =====\n\nexport const llmRetryHook = {\n  id: 'agentic-llm-retry',\n  type: 'llm-retry' as const,\n  priority: 90,\n  handler: async (\n    payload: LLMHookPayload,\n    context: AgenticHookContext\n  ): Promise<HookHandlerResult> => {\n    const { provider, model, metrics } = payload;\n    const retryCount = metrics?.retryCount || 0;\n    \n    // Adjust request parameters for retry\n    const adjustedRequest = adjustRequestForRetry(\n      payload.request,\n      retryCount\n    );\n    \n    const sideEffects: SideEffect[] = [\n      {\n        type: 'metric',\n        action: 'increment',\n        data: { name: `llm.retries.${provider}.${model}` },\n      },\n    ];\n    \n    // Apply exponential backoff\n    const backoffMs = Math.min(1000 * Math.pow(2, retryCount), 10000);\n    await new Promise(resolve => setTimeout(resolve, backoffMs));\n    \n    return {\n      continue: true,\n      modified: true,\n      payload: {\n        ...payload,\n        request: adjustedRequest,\n        metrics: {\n          ...metrics,\n          retryCount: retryCount + 1,\n        },\n      },\n      sideEffects,\n    };\n  },\n};\n\n// ===== Helper Functions =====\n\nfunction generateCacheKey(\n  provider: string,\n  model: string,\n  request: LLMHookPayload['request']\n): string {\n  const normalized = {\n    provider,\n    model,\n    messages: request.messages?.map(m => ({\n      role: m.role,\n      content: m.content.substring(0, 100), // First 100 chars\n    })),\n    temperature: request.temperature,\n    maxTokens: request.maxTokens,\n  };\n  \n  return Buffer.from(JSON.stringify(normalized)).toString('base64');\n}\n\nasync function checkMemoryCache(\n  cacheKey: string,\n  context: AgenticHookContext\n): Promise<any | null> {\n  // Implementation would integrate with memory service\n  // This is a placeholder\n  return null;\n}\n\nasync function loadProviderOptimizations(\n  provider: string,\n  context: AgenticHookContext\n): Promise<any> {\n  // Load provider-specific optimizations from memory\n  // This is a placeholder\n  return {\n    maxRetries: 3,\n    timeout: 30000,\n    rateLimit: 100,\n  };\n}\n\nfunction applyRequestOptimizations(\n  request: LLMHookPayload['request'],\n  optimizations: any,\n  context: AgenticHookContext\n): LLMHookPayload['request'] {\n  // Apply various optimizations\n  const optimized = { ...request };\n  \n  // Optimize token usage\n  if (optimized.maxTokens && optimized.maxTokens > 4000) {\n    optimized.maxTokens = 4000; // Cap at reasonable limit\n  }\n  \n  // Optimize temperature for consistency\n  if (optimized.temperature === undefined) {\n    optimized.temperature = 0.7;\n  }\n  \n  // Add stop sequences if missing\n  if (!optimized.stopSequences && optimized.messages) {\n    optimized.stopSequences = ['\\n\\nHuman:', '\\n\\nAssistant:'];\n  }\n  \n  return optimized;\n}\n\nfunction determineCacheTTL(\n  operation: string,\n  response: LLMHookPayload['response']\n): number {\n  // Determine cache TTL based on operation and response\n  switch (operation) {\n    case 'embedding':\n      return 86400; // 24 hours for embeddings\n    case 'completion':\n      // Shorter TTL for completions\n      return response?.usage?.totalTokens && response.usage.totalTokens > 1000\n        ? 1800 // 30 minutes for long responses\n        : 3600; // 1 hour for short responses\n    default:\n      return 3600; // 1 hour default\n  }\n}\n\nfunction extractResponsePatterns(\n  request: LLMHookPayload['request'],\n  response: LLMHookPayload['response'],\n  metrics: LLMMetrics\n): Pattern[] {\n  const patterns: Pattern[] = [];\n  \n  // Extract performance patterns\n  if (metrics.latency > 1000) {\n    patterns.push({\n      id: `perf_${Date.now()}`,\n      type: 'optimization',\n      confidence: 0.8,\n      occurrences: 1,\n      context: {\n        provider: metrics.providerHealth < 0.8 ? 'unhealthy' : 'healthy',\n        requestSize: JSON.stringify(request).length,\n        responseTokens: response?.usage?.totalTokens || 0,\n        latency: metrics.latency,\n      },\n    });\n  }\n  \n  // Extract success patterns\n  if (response?.choices?.[0]?.finishReason === 'stop') {\n    patterns.push({\n      id: `success_${Date.now()}`,\n      type: 'success',\n      confidence: 0.9,\n      occurrences: 1,\n      context: {\n        temperature: request.temperature,\n        maxTokens: request.maxTokens,\n        actualTokens: response.usage?.totalTokens || 0,\n      },\n    });\n  }\n  \n  return patterns;\n}\n\nfunction getLatencyThreshold(provider: string, model: string): number {\n  // Provider/model specific thresholds\n  const thresholds: Record<string, number> = {\n    'openai:gpt-4': 5000,\n    'openai:gpt-3.5-turbo': 2000,\n    'anthropic:claude-3': 4000,\n    'anthropic:claude-instant': 1500,\n  };\n  \n  return thresholds[`${provider}:${model}`] || 3000;\n}\n\nasync function updateProviderHealth(\n  provider: string,\n  health: number,\n  context: AgenticHookContext\n): Promise<void> {\n  // Update provider health in memory\n  const healthKey = `provider:health:${provider}`;\n  const currentHealth = await context.memory.cache.get(healthKey) || [];\n  \n  currentHealth.push({\n    timestamp: Date.now(),\n    health,\n  });\n  \n  // Keep last 100 health checks\n  if (currentHealth.length > 100) {\n    currentHealth.shift();\n  }\n  \n  await context.memory.cache.set(healthKey, currentHealth);\n}\n\nasync function selectFallbackProvider(\n  provider: string,\n  model: string,\n  error: Error,\n  context: AgenticHookContext\n): Promise<{ provider: string; model: string } | null> {\n  // Implement intelligent fallback selection\n  const fallbacks: Record<string, { provider: string; model: string }[]> = {\n    'openai': [\n      { provider: 'anthropic', model: 'claude-3' },\n      { provider: 'cohere', model: 'command' },\n    ],\n    'anthropic': [\n      { provider: 'openai', model: 'gpt-4' },\n      { provider: 'cohere', model: 'command' },\n    ],\n  };\n  \n  const candidates = fallbacks[provider] || [];\n  \n  // Select based on health scores\n  for (const candidate of candidates) {\n    const healthKey = `provider:health:${candidate.provider}`;\n    const healthData = await context.memory.cache.get(healthKey) || [];\n    \n    if (healthData.length > 0) {\n      const avgHealth = healthData.reduce((sum: number, h: any) => \n        sum + h.health, 0\n      ) / healthData.length;\n      \n      if (avgHealth > 0.7) {\n        return candidate;\n      }\n    }\n  }\n  \n  return null;\n}\n\nfunction adjustRequestForRetry(\n  request: LLMHookPayload['request'],\n  retryCount: number\n): LLMHookPayload['request'] {\n  const adjusted = { ...request };\n  \n  // Increase temperature slightly for variety\n  if (adjusted.temperature !== undefined) {\n    adjusted.temperature = Math.min(\n      adjusted.temperature + (0.1 * retryCount),\n      1.0\n    );\n  }\n  \n  // Reduce max tokens to improve success rate\n  if (adjusted.maxTokens !== undefined) {\n    adjusted.maxTokens = Math.floor(\n      adjusted.maxTokens * Math.pow(0.9, retryCount)\n    );\n  }\n  \n  return adjusted;\n}\n\n// ===== Register Hooks =====\n\nexport function registerLLMHooks(): void {\n  agenticHookManager.register(preLLMCallHook);\n  agenticHookManager.register(postLLMCallHook);\n  agenticHookManager.register(llmErrorHook);\n  agenticHookManager.register(llmRetryHook);\n}"],"names":["agenticHookManager","preLLMCallHook","id","type","priority","handler","payload","context","provider","model","operation","request","cacheKey","generateCacheKey","cached","checkMemoryCache","continue","modified","response","metrics","cacheHit","sideEffects","action","data","name","optimizations","loadProviderOptimizations","optimizedRequest","applyRequestOptimizations","key","correlationId","value","timestamp","Date","now","ttl","postLLMCallHook","push","determineCacheTTL","patterns","extractResponsePatterns","length","modelId","latency","usage","totalTokens","costEstimate","getLatencyThreshold","level","message","updateProviderHealth","providerHealth","llmErrorHook","error","stack","fallbackProvider","selectFallbackProvider","undefined","llmRetryHook","retryCount","adjustedRequest","adjustRequestForRetry","backoffMs","Math","min","pow","Promise","resolve","setTimeout","normalized","messages","map","m","role","content","substring","temperature","maxTokens","Buffer","from","JSON","stringify","toString","maxRetries","timeout","rateLimit","optimized","stopSequences","confidence","occurrences","requestSize","responseTokens","choices","finishReason","actualTokens","thresholds","health","healthKey","currentHealth","memory","cache","get","shift","set","fallbacks","candidates","candidate","healthData","avgHealth","reduce","sum","h","adjusted","floor","registerLLMHooks","register"],"mappings":"AAOA,SAASA,kBAAkB,QAAQ,oBAAoB;AAYvD,OAAO,MAAMC,iBAAiB;IAC5BC,IAAI;IACJC,MAAM;IACNC,UAAU;IACVC,SAAS,OACPC,SACAC;QAEA,MAAM,EAAEC,QAAQ,EAAEC,KAAK,EAAEC,WAAAA,UAAS,EAAEC,OAAO,EAAE,GAAGL;QAGhD,MAAMM,WAAWC,iBAAiBL,UAAUC,OAAOE;QACnD,MAAMG,SAAS,MAAMC,iBAAiBH,UAAUL;QAEhD,IAAIO,QAAQ;YACV,OAAO;gBACLE,UAAU;gBACVC,UAAU;gBACVX,SAAS;oBACP,GAAGA,OAAO;oBACVY,UAAUJ,OAAOI,QAAQ;oBACzBC,SAAS;wBACP,GAAGL,OAAOK,OAAO;wBACjBC,UAAU;oBACZ;gBACF;gBACAC,aAAa;oBACX;wBACElB,MAAM;wBACNmB,QAAQ;wBACRC,MAAM;4BAAEC,MAAM;wBAAiB;oBACjC;iBACD;YACH;QACF;QAGA,MAAMC,gBAAgB,MAAMC,0BAA0BlB,UAAUD;QAGhE,MAAMoB,mBAAmBC,0BACvBjB,SACAc,eACAlB;QAIF,MAAMc,cAA4B;YAChC;gBACElB,MAAM;gBACNmB,QAAQ;gBACRC,MAAM;oBAAEC,MAAM,CAAC,UAAU,EAAEhB,SAAS,CAAC,EAAEC,OAAO;gBAAC;YACjD;YACA;gBACEN,MAAM;gBACNmB,QAAQ;gBACRC,MAAM;oBACJM,KAAK,CAAC,YAAY,EAAEtB,QAAQuB,aAAa,EAAE;oBAC3CC,OAAO;wBACLvB;wBACAC;wBACAC,WAAAA;wBACAC,SAASgB;wBACTK,WAAWC,KAAKC,GAAG;oBACrB;oBACAC,KAAK;gBACP;YACF;SACD;QAED,OAAO;YACLnB,UAAU;YACVC,UAAU;YACVX,SAAS;gBACP,GAAGA,OAAO;gBACVK,SAASgB;YACX;YACAN;QACF;IACF;AACF,EAAE;AAIF,OAAO,MAAMe,kBAAkB;IAC7BlC,IAAI;IACJC,MAAM;IACNC,UAAU;IACVC,SAAS,OACPC,SACAC;QAEA,MAAM,EAAEC,QAAQ,EAAEC,KAAK,EAAEE,OAAO,EAAEO,QAAQ,EAAEC,OAAO,EAAE,GAAGb;QAExD,IAAI,CAACY,YAAY,CAACC,SAAS;YACzB,OAAO;gBAAEH,UAAU;YAAK;QAC1B;QAEA,MAAMK,cAA4B,EAAE;QAGpC,MAAMT,WAAWC,iBAAiBL,UAAUC,OAAOE;QACnDU,YAAYgB,IAAI,CAAC;YACflC,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBACJM,KAAK,CAAC,UAAU,EAAEjB,UAAU;gBAC5BmB,OAAO;oBACLb;oBACAC;oBACAa,WAAWC,KAAKC,GAAG;gBACrB;gBACAC,KAAKG,kBAAkB5B,WAAWQ;YACpC;QACF;QAGA,MAAMqB,WAAWC,wBAAwB7B,SAASO,UAAUC;QAC5D,IAAIoB,SAASE,MAAM,GAAG,GAAG;YACvBpB,YAAYgB,IAAI,CAAC;gBACflC,MAAM;gBACNmB,QAAQ;gBACRC,MAAM;oBACJgB;oBACAG,SAAS,CAAC,cAAc,EAAElC,UAAU;gBACtC;YACF;QACF;QAGAa,YAAYgB,IAAI,CACd;YACElC,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBACJC,MAAM,CAAC,YAAY,EAAEhB,SAAS,CAAC,EAAEC,OAAO;gBACxCsB,OAAOZ,QAAQwB,OAAO;YACxB;QACF,GACA;YACExC,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBACJC,MAAM,CAAC,WAAW,EAAEhB,SAAS,CAAC,EAAEC,OAAO;gBACvCsB,OAAOb,SAAS0B,KAAK,CAACC,WAAW;YACnC;QACF,GACA;YACE1C,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBACJC,MAAM,CAAC,SAAS,EAAEhB,SAAS,CAAC,EAAEC,OAAO;gBACrCsB,OAAOZ,QAAQ2B,YAAY;YAC7B;QACF;QAIF,IAAI3B,QAAQwB,OAAO,GAAGI,oBAAoBvC,UAAUC,QAAQ;YAC1DY,YAAYgB,IAAI,CAAC;gBACflC,MAAM;gBACNmB,QAAQ;gBACRC,MAAM;oBACJyB,OAAO;oBACPC,SAAS,CAAC,0BAA0B,EAAEzC,SAAS,CAAC,EAAEC,MAAM,EAAE,EAAEU,QAAQwB,OAAO,CAAC,EAAE,CAAC;gBACjF;YACF;QACF;QAGA,MAAMO,qBAAqB1C,UAAUW,QAAQgC,cAAc,EAAE5C;QAE7D,OAAO;YACLS,UAAU;YACVK;QACF;IACF;AACF,EAAE;AAIF,OAAO,MAAM+B,eAAe;IAC1BlD,IAAI;IACJC,MAAM;IACNC,UAAU;IACVC,SAAS,OACPC,SACAC;QAEA,MAAM,EAAEC,QAAQ,EAAEC,KAAK,EAAE4C,KAAK,EAAE,GAAG/C;QAEnC,IAAI,CAAC+C,OAAO;YACV,OAAO;gBAAErC,UAAU;YAAK;QAC1B;QAEA,MAAMK,cAA4B,EAAE;QAGpCA,YAAYgB,IAAI,CAAC;YACflC,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBACJyB,OAAO;gBACPC,SAAS,CAAC,eAAe,EAAEzC,SAAS,CAAC,EAAEC,OAAO;gBAC9Cc,MAAM;oBACJ8B,OAAOA,MAAMJ,OAAO;oBACpBK,OAAOD,MAAMC,KAAK;oBAClB3C,SAASL,QAAQK,OAAO;gBAC1B;YACF;QACF;QAGAU,YAAYgB,IAAI,CAAC;YACflC,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBAAEC,MAAM,CAAC,WAAW,EAAEhB,SAAS,CAAC,EAAEC,OAAO;YAAC;QAClD;QAGA,MAAM8C,mBAAmB,MAAMC,uBAC7BhD,UACAC,OACA4C,OACA9C;QAGF,IAAIgD,kBAAkB;YACpB,OAAO;gBACLvC,UAAU;gBACVC,UAAU;gBACVX,SAAS;oBACP,GAAGA,OAAO;oBACVE,UAAU+C,iBAAiB/C,QAAQ;oBACnCC,OAAO8C,iBAAiB9C,KAAK;oBAC7B4C,OAAOI;gBACT;gBACApC,aAAa;uBACRA;oBACH;wBACElB,MAAM;wBACNmB,QAAQ;wBACRC,MAAM;4BACJyB,OAAO;4BACPC,SAAS,CAAC,kBAAkB,EAAEzC,SAAS,CAAC,EAAEC,MAAM,IAAI,EAAE8C,iBAAiB/C,QAAQ,CAAC,CAAC,EAAE+C,iBAAiB9C,KAAK,EAAE;wBAC7G;oBACF;iBACD;YACH;QACF;QAEA,OAAO;YACLO,UAAU;YACVK;QACF;IACF;AACF,EAAE;AAIF,OAAO,MAAMqC,eAAe;IAC1BxD,IAAI;IACJC,MAAM;IACNC,UAAU;IACVC,SAAS,OACPC,SACAC;QAEA,MAAM,EAAEC,QAAQ,EAAEC,KAAK,EAAEU,OAAO,EAAE,GAAGb;QACrC,MAAMqD,aAAaxC,SAASwC,cAAc;QAG1C,MAAMC,kBAAkBC,sBACtBvD,QAAQK,OAAO,EACfgD;QAGF,MAAMtC,cAA4B;YAChC;gBACElB,MAAM;gBACNmB,QAAQ;gBACRC,MAAM;oBAAEC,MAAM,CAAC,YAAY,EAAEhB,SAAS,CAAC,EAAEC,OAAO;gBAAC;YACnD;SACD;QAGD,MAAMqD,YAAYC,KAAKC,GAAG,CAAC,OAAOD,KAAKE,GAAG,CAAC,GAAGN,aAAa;QAC3D,MAAM,IAAIO,QAAQC,CAAAA,UAAWC,WAAWD,SAASL;QAEjD,OAAO;YACL9C,UAAU;YACVC,UAAU;YACVX,SAAS;gBACP,GAAGA,OAAO;gBACVK,SAASiD;gBACTzC,SAAS;oBACP,GAAGA,OAAO;oBACVwC,YAAYA,aAAa;gBAC3B;YACF;YACAtC;QACF;IACF;AACF,EAAE;AAIF,SAASR,iBACPL,QAAgB,EAChBC,KAAa,EACbE,OAAkC;IAElC,MAAM0D,aAAa;QACjB7D;QACAC;QACA6D,UAAU3D,QAAQ2D,QAAQ,EAAEC,IAAIC,CAAAA,IAAM,CAAA;gBACpCC,MAAMD,EAAEC,IAAI;gBACZC,SAASF,EAAEE,OAAO,CAACC,SAAS,CAAC,GAAG;YAClC,CAAA;QACAC,aAAajE,QAAQiE,WAAW;QAChCC,WAAWlE,QAAQkE,SAAS;IAC9B;IAEA,OAAOC,OAAOC,IAAI,CAACC,KAAKC,SAAS,CAACZ,aAAaa,QAAQ,CAAC;AAC1D;AAEA,eAAenE,iBACbH,QAAgB,EAChBL,OAA2B;IAI3B,OAAO;AACT;AAEA,eAAemB,0BACblB,QAAgB,EAChBD,OAA2B;IAI3B,OAAO;QACL4E,YAAY;QACZC,SAAS;QACTC,WAAW;IACb;AACF;AAEA,SAASzD,0BACPjB,OAAkC,EAClCc,aAAkB,EAClBlB,OAA2B;IAG3B,MAAM+E,YAAY;QAAE,GAAG3E,OAAO;IAAC;IAG/B,IAAI2E,UAAUT,SAAS,IAAIS,UAAUT,SAAS,GAAG,MAAM;QACrDS,UAAUT,SAAS,GAAG;IACxB;IAGA,IAAIS,UAAUV,WAAW,KAAKnB,WAAW;QACvC6B,UAAUV,WAAW,GAAG;IAC1B;IAGA,IAAI,CAACU,UAAUC,aAAa,IAAID,UAAUhB,QAAQ,EAAE;QAClDgB,UAAUC,aAAa,GAAG;YAAC;YAAc;SAAiB;IAC5D;IAEA,OAAOD;AACT;AAEA,SAAShD,kBACP5B,UAAiB,EACjBQ,QAAoC;IAGpC,OAAQR;QACN,KAAK;YACH,OAAO;QACT,KAAK;YAEH,OAAOQ,UAAU0B,OAAOC,eAAe3B,SAAS0B,KAAK,CAACC,WAAW,GAAG,OAChE,OACA;QACN;YACE,OAAO;IACX;AACF;AAEA,SAASL,wBACP7B,OAAkC,EAClCO,QAAoC,EACpCC,OAAmB;IAEnB,MAAMoB,WAAsB,EAAE;IAG9B,IAAIpB,QAAQwB,OAAO,GAAG,MAAM;QAC1BJ,SAASF,IAAI,CAAC;YACZnC,IAAI,CAAC,KAAK,EAAE+B,KAAKC,GAAG,IAAI;YACxB/B,MAAM;YACNqF,YAAY;YACZC,aAAa;YACblF,SAAS;gBACPC,UAAUW,QAAQgC,cAAc,GAAG,MAAM,cAAc;gBACvDuC,aAAaV,KAAKC,SAAS,CAACtE,SAAS8B,MAAM;gBAC3CkD,gBAAgBzE,UAAU0B,OAAOC,eAAe;gBAChDF,SAASxB,QAAQwB,OAAO;YAC1B;QACF;IACF;IAGA,IAAIzB,UAAU0E,SAAS,CAAC,EAAE,EAAEC,iBAAiB,QAAQ;QACnDtD,SAASF,IAAI,CAAC;YACZnC,IAAI,CAAC,QAAQ,EAAE+B,KAAKC,GAAG,IAAI;YAC3B/B,MAAM;YACNqF,YAAY;YACZC,aAAa;YACblF,SAAS;gBACPqE,aAAajE,QAAQiE,WAAW;gBAChCC,WAAWlE,QAAQkE,SAAS;gBAC5BiB,cAAc5E,SAAS0B,KAAK,EAAEC,eAAe;YAC/C;QACF;IACF;IAEA,OAAON;AACT;AAEA,SAASQ,oBAAoBvC,QAAgB,EAAEC,KAAa;IAE1D,MAAMsF,aAAqC;QACzC,gBAAgB;QAChB,wBAAwB;QACxB,sBAAsB;QACtB,4BAA4B;IAC9B;IAEA,OAAOA,UAAU,CAAC,GAAGvF,SAAS,CAAC,EAAEC,OAAO,CAAC,IAAI;AAC/C;AAEA,eAAeyC,qBACb1C,QAAgB,EAChBwF,MAAc,EACdzF,OAA2B;IAG3B,MAAM0F,YAAY,CAAC,gBAAgB,EAAEzF,UAAU;IAC/C,MAAM0F,gBAAgB,MAAM3F,QAAQ4F,MAAM,CAACC,KAAK,CAACC,GAAG,CAACJ,cAAc,EAAE;IAErEC,cAAc7D,IAAI,CAAC;QACjBL,WAAWC,KAAKC,GAAG;QACnB8D;IACF;IAGA,IAAIE,cAAczD,MAAM,GAAG,KAAK;QAC9ByD,cAAcI,KAAK;IACrB;IAEA,MAAM/F,QAAQ4F,MAAM,CAACC,KAAK,CAACG,GAAG,CAACN,WAAWC;AAC5C;AAEA,eAAe1C,uBACbhD,QAAgB,EAChBC,KAAa,EACb4C,KAAY,EACZ9C,OAA2B;IAG3B,MAAMiG,YAAmE;QACvE,UAAU;YACR;gBAAEhG,UAAU;gBAAaC,OAAO;YAAW;YAC3C;gBAAED,UAAU;gBAAUC,OAAO;YAAU;SACxC;QACD,aAAa;YACX;gBAAED,UAAU;gBAAUC,OAAO;YAAQ;YACrC;gBAAED,UAAU;gBAAUC,OAAO;YAAU;SACxC;IACH;IAEA,MAAMgG,aAAaD,SAAS,CAAChG,SAAS,IAAI,EAAE;IAG5C,KAAK,MAAMkG,aAAaD,WAAY;QAClC,MAAMR,YAAY,CAAC,gBAAgB,EAAES,UAAUlG,QAAQ,EAAE;QACzD,MAAMmG,aAAa,MAAMpG,QAAQ4F,MAAM,CAACC,KAAK,CAACC,GAAG,CAACJ,cAAc,EAAE;QAElE,IAAIU,WAAWlE,MAAM,GAAG,GAAG;YACzB,MAAMmE,YAAYD,WAAWE,MAAM,CAAC,CAACC,KAAaC,IAChDD,MAAMC,EAAEf,MAAM,EAAE,KACdW,WAAWlE,MAAM;YAErB,IAAImE,YAAY,KAAK;gBACnB,OAAOF;YACT;QACF;IACF;IAEA,OAAO;AACT;AAEA,SAAS7C,sBACPlD,OAAkC,EAClCgD,UAAkB;IAElB,MAAMqD,WAAW;QAAE,GAAGrG,OAAO;IAAC;IAG9B,IAAIqG,SAASpC,WAAW,KAAKnB,WAAW;QACtCuD,SAASpC,WAAW,GAAGb,KAAKC,GAAG,CAC7BgD,SAASpC,WAAW,GAAI,MAAMjB,YAC9B;IAEJ;IAGA,IAAIqD,SAASnC,SAAS,KAAKpB,WAAW;QACpCuD,SAASnC,SAAS,GAAGd,KAAKkD,KAAK,CAC7BD,SAASnC,SAAS,GAAGd,KAAKE,GAAG,CAAC,KAAKN;IAEvC;IAEA,OAAOqD;AACT;AAIA,OAAO,SAASE;IACdlH,mBAAmBmH,QAAQ,CAAClH;IAC5BD,mBAAmBmH,QAAQ,CAAC/E;IAC5BpC,mBAAmBmH,QAAQ,CAAC/D;IAC5BpD,mBAAmBmH,QAAQ,CAACzD;AAC9B"}