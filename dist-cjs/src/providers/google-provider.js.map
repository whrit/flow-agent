{"version":3,"sources":["../../../src/providers/google-provider.ts"],"sourcesContent":["/**\n * Google AI Provider Implementation\n * Supports Gemini Pro, PaLM, and other Google models\n */\n\nimport { BaseProvider } from './base-provider.js';\nimport {\n  LLMProvider,\n  LLMModel,\n  LLMRequest,\n  LLMResponse,\n  LLMStreamEvent,\n  ModelInfo,\n  ProviderCapabilities,\n  HealthCheckResult,\n  LLMProviderError,\n  RateLimitError,\n  AuthenticationError,\n} from './types.js';\n\ninterface GoogleAIRequest {\n  contents: Array<{\n    role: 'user' | 'model';\n    parts: Array<{\n      text?: string;\n      inlineData?: {\n        mimeType: string;\n        data: string;\n      };\n    }>;\n  }>;\n  generationConfig?: {\n    temperature?: number;\n    topK?: number;\n    topP?: number;\n    maxOutputTokens?: number;\n    stopSequences?: string[];\n  };\n  safetySettings?: Array<{\n    category: string;\n    threshold: string;\n  }>;\n}\n\ninterface GoogleAIResponse {\n  candidates: Array<{\n    content: {\n      parts: Array<{\n        text: string;\n      }>;\n      role: string;\n    };\n    finishReason: string;\n    index: number;\n    safetyRatings: Array<{\n      category: string;\n      probability: string;\n    }>;\n  }>;\n  promptFeedback?: {\n    safetyRatings: Array<{\n      category: string;\n      probability: string;\n    }>;\n  };\n  usageMetadata?: {\n    promptTokenCount: number;\n    candidatesTokenCount: number;\n    totalTokenCount: number;\n  };\n}\n\nexport class GoogleProvider extends BaseProvider {\n  readonly name: LLMProvider = 'google';\n  readonly capabilities: ProviderCapabilities = {\n    supportedModels: [\n      'gemini-pro',\n      'gemini-pro-vision',\n      'palm-2',\n      'bison',\n    ],\n    maxContextLength: {\n      'gemini-pro': 32768,\n      'gemini-pro-vision': 16384,\n      'palm-2': 8192,\n      'bison': 4096,\n    } as Record<LLMModel, number>,\n    maxOutputTokens: {\n      'gemini-pro': 2048,\n      'gemini-pro-vision': 2048,\n      'palm-2': 1024,\n      'bison': 1024,\n    } as Record<LLMModel, number>,\n    supportsStreaming: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: false, // Google AI doesn't have explicit system messages\n    supportsVision: true, // Gemini Pro Vision\n    supportsAudio: false,\n    supportsTools: true,\n    supportsFineTuning: false,\n    supportsEmbeddings: true,\n    supportsLogprobs: false,\n    supportsBatching: true,\n    rateLimit: {\n      requestsPerMinute: 60,\n      tokensPerMinute: 60000,\n      concurrentRequests: 10,\n    },\n    pricing: {\n      'gemini-pro': {\n        promptCostPer1k: 0.00025,\n        completionCostPer1k: 0.0005,\n        currency: 'USD',\n      },\n      'gemini-pro-vision': {\n        promptCostPer1k: 0.00025,\n        completionCostPer1k: 0.0005,\n        currency: 'USD',\n      },\n      'palm-2': {\n        promptCostPer1k: 0.0005,\n        completionCostPer1k: 0.001,\n        currency: 'USD',\n      },\n      'bison': {\n        promptCostPer1k: 0.0005,\n        completionCostPer1k: 0.001,\n        currency: 'USD',\n      },\n    },\n  };\n\n  private baseUrl: string;\n\n  protected async doInitialize(): Promise<void> {\n    if (!this.config.apiKey) {\n      throw new AuthenticationError('Google AI API key is required', 'google');\n    }\n\n    // Use Gemini API for newer models, PaLM API for older ones\n    const model = this.config.model;\n    if (model.startsWith('gemini')) {\n      this.baseUrl = 'https://generativelanguage.googleapis.com/v1beta';\n    } else {\n      this.baseUrl = 'https://generativelanguage.googleapis.com/v1beta2';\n    }\n  }\n\n  protected async doComplete(request: LLMRequest): Promise<LLMResponse> {\n    const googleRequest = this.buildGoogleRequest(request);\n    const model = this.mapToGoogleModel(request.model || this.config.model);\n    \n    const url = `${this.baseUrl}/models/${model}:generateContent?key=${this.config.apiKey}`;\n    \n    const controller = new AbortController();\n    const timeout = setTimeout(() => controller.abort(), this.config.timeout || 60000);\n\n    try {\n      const response = await fetch(url, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify(googleRequest),\n        signal: controller.signal,\n      });\n\n      clearTimeout(timeout);\n\n      if (!response.ok) {\n        await this.handleErrorResponse(response);\n      }\n\n      const data: GoogleAIResponse = await response.json();\n      \n      if (!data.candidates || data.candidates.length === 0) {\n        throw new LLMProviderError(\n          'No response generated',\n          'NO_RESPONSE',\n          'google',\n          undefined,\n          false\n        );\n      }\n\n      const candidate = data.candidates[0];\n      const content = candidate.content.parts.map(part => part.text).join('');\n      \n      // Calculate cost\n      const usageData = data.usageMetadata || {\n        promptTokenCount: this.estimateTokens(JSON.stringify(request.messages)),\n        candidatesTokenCount: this.estimateTokens(content),\n        totalTokenCount: 0,\n      };\n      usageData.totalTokenCount = usageData.promptTokenCount + usageData.candidatesTokenCount;\n\n      const pricing = this.capabilities.pricing![request.model || this.config.model];\n      const promptCost = (usageData.promptTokenCount / 1000) * pricing.promptCostPer1k;\n      const completionCost = (usageData.candidatesTokenCount / 1000) * pricing.completionCostPer1k;\n\n      return {\n        id: `google-${Date.now()}`,\n        model: request.model || this.config.model,\n        provider: 'google',\n        content,\n        usage: {\n          promptTokens: usageData.promptTokenCount,\n          completionTokens: usageData.candidatesTokenCount,\n          totalTokens: usageData.totalTokenCount,\n        },\n        cost: {\n          promptCost,\n          completionCost,\n          totalCost: promptCost + completionCost,\n          currency: 'USD',\n        },\n        finishReason: this.mapFinishReason(candidate.finishReason),\n      };\n    } catch (error) {\n      clearTimeout(timeout);\n      throw this.transformError(error);\n    }\n  }\n\n  protected async *doStreamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent> {\n    const googleRequest = this.buildGoogleRequest(request);\n    const model = this.mapToGoogleModel(request.model || this.config.model);\n    \n    const url = `${this.baseUrl}/models/${model}:streamGenerateContent?key=${this.config.apiKey}`;\n    \n    const controller = new AbortController();\n    const timeout = setTimeout(() => controller.abort(), (this.config.timeout || 60000) * 2);\n\n    try {\n      const response = await fetch(url, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify(googleRequest),\n        signal: controller.signal,\n      });\n\n      if (!response.ok) {\n        await this.handleErrorResponse(response);\n      }\n\n      const reader = response.body!.getReader();\n      const decoder = new TextDecoder();\n      let buffer = '';\n      let totalContent = '';\n      let promptTokens = 0;\n      let completionTokens = 0;\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          if (line.trim() === '') continue;\n          \n          try {\n            const data: GoogleAIResponse = JSON.parse(line);\n            \n            if (data.candidates && data.candidates.length > 0) {\n              const candidate = data.candidates[0];\n              const content = candidate.content.parts.map(part => part.text).join('');\n              \n              if (content) {\n                totalContent += content;\n                yield {\n                  type: 'content',\n                  delta: { content },\n                };\n              }\n              \n              if (data.usageMetadata) {\n                promptTokens = data.usageMetadata.promptTokenCount;\n                completionTokens = data.usageMetadata.candidatesTokenCount;\n              }\n            }\n          } catch (e) {\n            this.logger.warn('Failed to parse Google AI stream chunk', { line, error: e });\n          }\n        }\n      }\n\n      // Final event with usage and cost\n      const pricing = this.capabilities.pricing![request.model || this.config.model];\n      const promptCost = (promptTokens / 1000) * pricing.promptCostPer1k;\n      const completionCost = (completionTokens / 1000) * pricing.completionCostPer1k;\n\n      yield {\n        type: 'done',\n        usage: {\n          promptTokens,\n          completionTokens,\n          totalTokens: promptTokens + completionTokens,\n        },\n        cost: {\n          promptCost,\n          completionCost,\n          totalCost: promptCost + completionCost,\n          currency: 'USD',\n        },\n      };\n    } catch (error) {\n      clearTimeout(timeout);\n      throw this.transformError(error);\n    } finally {\n      clearTimeout(timeout);\n    }\n  }\n\n  async listModels(): Promise<LLMModel[]> {\n    return this.capabilities.supportedModels;\n  }\n\n  async getModelInfo(model: LLMModel): Promise<ModelInfo> {\n    return {\n      model,\n      name: model,\n      description: this.getModelDescription(model),\n      contextLength: this.capabilities.maxContextLength[model] || 4096,\n      maxOutputTokens: this.capabilities.maxOutputTokens[model] || 2048,\n      supportedFeatures: [\n        'chat',\n        'completion',\n        ...(model.includes('vision') ? ['vision'] : []),\n        ...(model.startsWith('gemini') ? ['function_calling'] : []),\n      ],\n      pricing: this.capabilities.pricing![model],\n    };\n  }\n\n  protected async doHealthCheck(): Promise<HealthCheckResult> {\n    try {\n      const url = `${this.baseUrl}/models?key=${this.config.apiKey}`;\n      const response = await fetch(url);\n\n      if (!response.ok) {\n        throw new Error(`Health check failed: ${response.status}`);\n      }\n\n      return {\n        healthy: true,\n        timestamp: new Date(),\n      };\n    } catch (error) {\n      return {\n        healthy: false,\n        error: error instanceof Error ? error.message : 'Unknown error',\n        timestamp: new Date(),\n      };\n    }\n  }\n\n  private buildGoogleRequest(request: LLMRequest): GoogleAIRequest {\n    // Convert messages to Google format\n    const contents: GoogleAIRequest['contents'] = [];\n    \n    for (const message of request.messages) {\n      // Skip system messages or prepend to first user message\n      if (message.role === 'system') {\n        if (contents.length === 0) {\n          contents.push({\n            role: 'user',\n            parts: [{ text: `Instructions: ${message.content}` }],\n          });\n        }\n        continue;\n      }\n      \n      contents.push({\n        role: message.role === 'assistant' ? 'model' : 'user',\n        parts: [{ text: message.content }],\n      });\n    }\n\n    return {\n      contents,\n      generationConfig: {\n        temperature: request.temperature ?? this.config.temperature,\n        topK: request.topK ?? this.config.topK,\n        topP: request.topP ?? this.config.topP,\n        maxOutputTokens: request.maxTokens ?? this.config.maxTokens,\n        stopSequences: request.stopSequences ?? this.config.stopSequences,\n      },\n      safetySettings: [\n        {\n          category: 'HARM_CATEGORY_HARASSMENT',\n          threshold: 'BLOCK_NONE',\n        },\n        {\n          category: 'HARM_CATEGORY_HATE_SPEECH',\n          threshold: 'BLOCK_NONE',\n        },\n        {\n          category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n          threshold: 'BLOCK_NONE',\n        },\n        {\n          category: 'HARM_CATEGORY_DANGEROUS_CONTENT',\n          threshold: 'BLOCK_NONE',\n        },\n      ],\n    };\n  }\n\n  private mapToGoogleModel(model: LLMModel): string {\n    const modelMap: Record<string, string> = {\n      'gemini-pro': 'gemini-pro',\n      'gemini-pro-vision': 'gemini-pro-vision',\n      'palm-2': 'text-bison-001',\n      'bison': 'text-bison-001',\n    };\n    return modelMap[model] || model;\n  }\n\n  private mapFinishReason(reason: string): 'stop' | 'length' | 'content_filter' {\n    switch (reason) {\n      case 'STOP':\n        return 'stop';\n      case 'MAX_TOKENS':\n        return 'length';\n      case 'SAFETY':\n      case 'RECITATION':\n        return 'content_filter';\n      default:\n        return 'stop';\n    }\n  }\n\n  private getModelDescription(model: LLMModel): string {\n    const descriptions: Record<string, string> = {\n      'gemini-pro': 'Google\\'s most capable text model',\n      'gemini-pro-vision': 'Gemini Pro with vision capabilities',\n      'palm-2': 'Previous generation large language model',\n      'bison': 'Efficient model for various tasks',\n    };\n    return descriptions[model] || 'Google AI language model';\n  }\n\n  private async handleErrorResponse(response: Response): Promise<void> {\n    const errorText = await response.text();\n    let errorData: any;\n\n    try {\n      errorData = JSON.parse(errorText);\n    } catch {\n      errorData = { error: { message: errorText } };\n    }\n\n    const message = errorData.error?.message || 'Unknown error';\n\n    switch (response.status) {\n      case 401:\n      case 403:\n        throw new AuthenticationError(message, 'google', errorData);\n      case 429:\n        throw new RateLimitError(message, 'google', undefined, errorData);\n      default:\n        throw new LLMProviderError(\n          message,\n          `GOOGLE_${response.status}`,\n          'google',\n          response.status,\n          response.status >= 500,\n          errorData\n        );\n    }\n  }\n}"],"names":["BaseProvider","LLMProviderError","RateLimitError","AuthenticationError","GoogleProvider","name","capabilities","supportedModels","maxContextLength","maxOutputTokens","supportsStreaming","supportsFunctionCalling","supportsSystemMessages","supportsVision","supportsAudio","supportsTools","supportsFineTuning","supportsEmbeddings","supportsLogprobs","supportsBatching","rateLimit","requestsPerMinute","tokensPerMinute","concurrentRequests","pricing","promptCostPer1k","completionCostPer1k","currency","baseUrl","doInitialize","config","apiKey","model","startsWith","doComplete","request","googleRequest","buildGoogleRequest","mapToGoogleModel","url","controller","AbortController","timeout","setTimeout","abort","response","fetch","method","headers","body","JSON","stringify","signal","clearTimeout","ok","handleErrorResponse","data","json","candidates","length","undefined","candidate","content","parts","map","part","text","join","usageData","usageMetadata","promptTokenCount","estimateTokens","messages","candidatesTokenCount","totalTokenCount","promptCost","completionCost","id","Date","now","provider","usage","promptTokens","completionTokens","totalTokens","cost","totalCost","finishReason","mapFinishReason","error","transformError","doStreamComplete","reader","getReader","decoder","TextDecoder","buffer","totalContent","done","value","read","decode","stream","lines","split","pop","line","trim","parse","type","delta","e","logger","warn","listModels","getModelInfo","description","getModelDescription","contextLength","supportedFeatures","includes","doHealthCheck","Error","status","healthy","timestamp","message","contents","role","push","generationConfig","temperature","topK","topP","maxTokens","stopSequences","safetySettings","category","threshold","modelMap","reason","descriptions","errorText","errorData"],"mappings":"AAKA,SAASA,YAAY,QAAQ,qBAAqB;AAClD,SASEC,gBAAgB,EAChBC,cAAc,EACdC,mBAAmB,QACd,aAAa;AAsDpB,OAAO,MAAMC,uBAAuBJ;IACzBK,OAAoB,SAAS;IAC7BC,eAAqC;QAC5CC,iBAAiB;YACf;YACA;YACA;YACA;SACD;QACDC,kBAAkB;YAChB,cAAc;YACd,qBAAqB;YACrB,UAAU;YACV,SAAS;QACX;QACAC,iBAAiB;YACf,cAAc;YACd,qBAAqB;YACrB,UAAU;YACV,SAAS;QACX;QACAC,mBAAmB;QACnBC,yBAAyB;QACzBC,wBAAwB;QACxBC,gBAAgB;QAChBC,eAAe;QACfC,eAAe;QACfC,oBAAoB;QACpBC,oBAAoB;QACpBC,kBAAkB;QAClBC,kBAAkB;QAClBC,WAAW;YACTC,mBAAmB;YACnBC,iBAAiB;YACjBC,oBAAoB;QACtB;QACAC,SAAS;YACP,cAAc;gBACZC,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;YACA,qBAAqB;gBACnBF,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;YACA,UAAU;gBACRF,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;YACA,SAAS;gBACPF,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;QACF;IACF,EAAE;IAEMC,QAAgB;IAExB,MAAgBC,eAA8B;QAC5C,IAAI,CAAC,IAAI,CAACC,MAAM,CAACC,MAAM,EAAE;YACvB,MAAM,IAAI5B,oBAAoB,iCAAiC;QACjE;QAGA,MAAM6B,QAAQ,IAAI,CAACF,MAAM,CAACE,KAAK;QAC/B,IAAIA,MAAMC,UAAU,CAAC,WAAW;YAC9B,IAAI,CAACL,OAAO,GAAG;QACjB,OAAO;YACL,IAAI,CAACA,OAAO,GAAG;QACjB;IACF;IAEA,MAAgBM,WAAWC,OAAmB,EAAwB;QACpE,MAAMC,gBAAgB,IAAI,CAACC,kBAAkB,CAACF;QAC9C,MAAMH,QAAQ,IAAI,CAACM,gBAAgB,CAACH,QAAQH,KAAK,IAAI,IAAI,CAACF,MAAM,CAACE,KAAK;QAEtE,MAAMO,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,QAAQ,EAAEI,MAAM,qBAAqB,EAAE,IAAI,CAACF,MAAM,CAACC,MAAM,EAAE;QAEvF,MAAMS,aAAa,IAAIC;QACvB,MAAMC,UAAUC,WAAW,IAAMH,WAAWI,KAAK,IAAI,IAAI,CAACd,MAAM,CAACY,OAAO,IAAI;QAE5E,IAAI;YACF,MAAMG,WAAW,MAAMC,MAAMP,KAAK;gBAChCQ,QAAQ;gBACRC,SAAS;oBACP,gBAAgB;gBAClB;gBACAC,MAAMC,KAAKC,SAAS,CAACf;gBACrBgB,QAAQZ,WAAWY,MAAM;YAC3B;YAEAC,aAAaX;YAEb,IAAI,CAACG,SAASS,EAAE,EAAE;gBAChB,MAAM,IAAI,CAACC,mBAAmB,CAACV;YACjC;YAEA,MAAMW,OAAyB,MAAMX,SAASY,IAAI;YAElD,IAAI,CAACD,KAAKE,UAAU,IAAIF,KAAKE,UAAU,CAACC,MAAM,KAAK,GAAG;gBACpD,MAAM,IAAI1D,iBACR,yBACA,eACA,UACA2D,WACA;YAEJ;YAEA,MAAMC,YAAYL,KAAKE,UAAU,CAAC,EAAE;YACpC,MAAMI,UAAUD,UAAUC,OAAO,CAACC,KAAK,CAACC,GAAG,CAACC,CAAAA,OAAQA,KAAKC,IAAI,EAAEC,IAAI,CAAC;YAGpE,MAAMC,YAAYZ,KAAKa,aAAa,IAAI;gBACtCC,kBAAkB,IAAI,CAACC,cAAc,CAACrB,KAAKC,SAAS,CAAChB,QAAQqC,QAAQ;gBACrEC,sBAAsB,IAAI,CAACF,cAAc,CAACT;gBAC1CY,iBAAiB;YACnB;YACAN,UAAUM,eAAe,GAAGN,UAAUE,gBAAgB,GAAGF,UAAUK,oBAAoB;YAEvF,MAAMjD,UAAU,IAAI,CAAClB,YAAY,CAACkB,OAAO,AAAC,CAACW,QAAQH,KAAK,IAAI,IAAI,CAACF,MAAM,CAACE,KAAK,CAAC;YAC9E,MAAM2C,aAAa,AAACP,UAAUE,gBAAgB,GAAG,OAAQ9C,QAAQC,eAAe;YAChF,MAAMmD,iBAAiB,AAACR,UAAUK,oBAAoB,GAAG,OAAQjD,QAAQE,mBAAmB;YAE5F,OAAO;gBACLmD,IAAI,CAAC,OAAO,EAAEC,KAAKC,GAAG,IAAI;gBAC1B/C,OAAOG,QAAQH,KAAK,IAAI,IAAI,CAACF,MAAM,CAACE,KAAK;gBACzCgD,UAAU;gBACVlB;gBACAmB,OAAO;oBACLC,cAAcd,UAAUE,gBAAgB;oBACxCa,kBAAkBf,UAAUK,oBAAoB;oBAChDW,aAAahB,UAAUM,eAAe;gBACxC;gBACAW,MAAM;oBACJV;oBACAC;oBACAU,WAAWX,aAAaC;oBACxBjD,UAAU;gBACZ;gBACA4D,cAAc,IAAI,CAACC,eAAe,CAAC3B,UAAU0B,YAAY;YAC3D;QACF,EAAE,OAAOE,OAAO;YACdpC,aAAaX;YACb,MAAM,IAAI,CAACgD,cAAc,CAACD;QAC5B;IACF;IAEA,OAAiBE,iBAAiBxD,OAAmB,EAAiC;QACpF,MAAMC,gBAAgB,IAAI,CAACC,kBAAkB,CAACF;QAC9C,MAAMH,QAAQ,IAAI,CAACM,gBAAgB,CAACH,QAAQH,KAAK,IAAI,IAAI,CAACF,MAAM,CAACE,KAAK;QAEtE,MAAMO,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,QAAQ,EAAEI,MAAM,2BAA2B,EAAE,IAAI,CAACF,MAAM,CAACC,MAAM,EAAE;QAE7F,MAAMS,aAAa,IAAIC;QACvB,MAAMC,UAAUC,WAAW,IAAMH,WAAWI,KAAK,IAAI,AAAC,CAAA,IAAI,CAACd,MAAM,CAACY,OAAO,IAAI,KAAI,IAAK;QAEtF,IAAI;YACF,MAAMG,WAAW,MAAMC,MAAMP,KAAK;gBAChCQ,QAAQ;gBACRC,SAAS;oBACP,gBAAgB;gBAClB;gBACAC,MAAMC,KAAKC,SAAS,CAACf;gBACrBgB,QAAQZ,WAAWY,MAAM;YAC3B;YAEA,IAAI,CAACP,SAASS,EAAE,EAAE;gBAChB,MAAM,IAAI,CAACC,mBAAmB,CAACV;YACjC;YAEA,MAAM+C,SAAS/C,SAASI,IAAI,CAAE4C,SAAS;YACvC,MAAMC,UAAU,IAAIC;YACpB,IAAIC,SAAS;YACb,IAAIC,eAAe;YACnB,IAAIf,eAAe;YACnB,IAAIC,mBAAmB;YAEvB,MAAO,KAAM;gBACX,MAAM,EAAEe,IAAI,EAAEC,KAAK,EAAE,GAAG,MAAMP,OAAOQ,IAAI;gBACzC,IAAIF,MAAM;gBAEVF,UAAUF,QAAQO,MAAM,CAACF,OAAO;oBAAEG,QAAQ;gBAAK;gBAC/C,MAAMC,QAAQP,OAAOQ,KAAK,CAAC;gBAC3BR,SAASO,MAAME,GAAG,MAAM;gBAExB,KAAK,MAAMC,QAAQH,MAAO;oBACxB,IAAIG,KAAKC,IAAI,OAAO,IAAI;oBAExB,IAAI;wBACF,MAAMnD,OAAyBN,KAAK0D,KAAK,CAACF;wBAE1C,IAAIlD,KAAKE,UAAU,IAAIF,KAAKE,UAAU,CAACC,MAAM,GAAG,GAAG;4BACjD,MAAME,YAAYL,KAAKE,UAAU,CAAC,EAAE;4BACpC,MAAMI,UAAUD,UAAUC,OAAO,CAACC,KAAK,CAACC,GAAG,CAACC,CAAAA,OAAQA,KAAKC,IAAI,EAAEC,IAAI,CAAC;4BAEpE,IAAIL,SAAS;gCACXmC,gBAAgBnC;gCAChB,MAAM;oCACJ+C,MAAM;oCACNC,OAAO;wCAAEhD;oCAAQ;gCACnB;4BACF;4BAEA,IAAIN,KAAKa,aAAa,EAAE;gCACtBa,eAAe1B,KAAKa,aAAa,CAACC,gBAAgB;gCAClDa,mBAAmB3B,KAAKa,aAAa,CAACI,oBAAoB;4BAC5D;wBACF;oBACF,EAAE,OAAOsC,GAAG;wBACV,IAAI,CAACC,MAAM,CAACC,IAAI,CAAC,0CAA0C;4BAAEP;4BAAMjB,OAAOsB;wBAAE;oBAC9E;gBACF;YACF;YAGA,MAAMvF,UAAU,IAAI,CAAClB,YAAY,CAACkB,OAAO,AAAC,CAACW,QAAQH,KAAK,IAAI,IAAI,CAACF,MAAM,CAACE,KAAK,CAAC;YAC9E,MAAM2C,aAAa,AAACO,eAAe,OAAQ1D,QAAQC,eAAe;YAClE,MAAMmD,iBAAiB,AAACO,mBAAmB,OAAQ3D,QAAQE,mBAAmB;YAE9E,MAAM;gBACJmF,MAAM;gBACN5B,OAAO;oBACLC;oBACAC;oBACAC,aAAaF,eAAeC;gBAC9B;gBACAE,MAAM;oBACJV;oBACAC;oBACAU,WAAWX,aAAaC;oBACxBjD,UAAU;gBACZ;YACF;QACF,EAAE,OAAO8D,OAAO;YACdpC,aAAaX;YACb,MAAM,IAAI,CAACgD,cAAc,CAACD;QAC5B,SAAU;YACRpC,aAAaX;QACf;IACF;IAEA,MAAMwE,aAAkC;QACtC,OAAO,IAAI,CAAC5G,YAAY,CAACC,eAAe;IAC1C;IAEA,MAAM4G,aAAanF,KAAe,EAAsB;QACtD,OAAO;YACLA;YACA3B,MAAM2B;YACNoF,aAAa,IAAI,CAACC,mBAAmB,CAACrF;YACtCsF,eAAe,IAAI,CAAChH,YAAY,CAACE,gBAAgB,CAACwB,MAAM,IAAI;YAC5DvB,iBAAiB,IAAI,CAACH,YAAY,CAACG,eAAe,CAACuB,MAAM,IAAI;YAC7DuF,mBAAmB;gBACjB;gBACA;mBACIvF,MAAMwF,QAAQ,CAAC,YAAY;oBAAC;iBAAS,GAAG,EAAE;mBAC1CxF,MAAMC,UAAU,CAAC,YAAY;oBAAC;iBAAmB,GAAG,EAAE;aAC3D;YACDT,SAAS,IAAI,CAAClB,YAAY,CAACkB,OAAO,AAAC,CAACQ,MAAM;QAC5C;IACF;IAEA,MAAgByF,gBAA4C;QAC1D,IAAI;YACF,MAAMlF,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,YAAY,EAAE,IAAI,CAACE,MAAM,CAACC,MAAM,EAAE;YAC9D,MAAMc,WAAW,MAAMC,MAAMP;YAE7B,IAAI,CAACM,SAASS,EAAE,EAAE;gBAChB,MAAM,IAAIoE,MAAM,CAAC,qBAAqB,EAAE7E,SAAS8E,MAAM,EAAE;YAC3D;YAEA,OAAO;gBACLC,SAAS;gBACTC,WAAW,IAAI/C;YACjB;QACF,EAAE,OAAOW,OAAO;YACd,OAAO;gBACLmC,SAAS;gBACTnC,OAAOA,iBAAiBiC,QAAQjC,MAAMqC,OAAO,GAAG;gBAChDD,WAAW,IAAI/C;YACjB;QACF;IACF;IAEQzC,mBAAmBF,OAAmB,EAAmB;QAE/D,MAAM4F,WAAwC,EAAE;QAEhD,KAAK,MAAMD,WAAW3F,QAAQqC,QAAQ,CAAE;YAEtC,IAAIsD,QAAQE,IAAI,KAAK,UAAU;gBAC7B,IAAID,SAASpE,MAAM,KAAK,GAAG;oBACzBoE,SAASE,IAAI,CAAC;wBACZD,MAAM;wBACNjE,OAAO;4BAAC;gCAAEG,MAAM,CAAC,cAAc,EAAE4D,QAAQhE,OAAO,EAAE;4BAAC;yBAAE;oBACvD;gBACF;gBACA;YACF;YAEAiE,SAASE,IAAI,CAAC;gBACZD,MAAMF,QAAQE,IAAI,KAAK,cAAc,UAAU;gBAC/CjE,OAAO;oBAAC;wBAAEG,MAAM4D,QAAQhE,OAAO;oBAAC;iBAAE;YACpC;QACF;QAEA,OAAO;YACLiE;YACAG,kBAAkB;gBAChBC,aAAahG,QAAQgG,WAAW,IAAI,IAAI,CAACrG,MAAM,CAACqG,WAAW;gBAC3DC,MAAMjG,QAAQiG,IAAI,IAAI,IAAI,CAACtG,MAAM,CAACsG,IAAI;gBACtCC,MAAMlG,QAAQkG,IAAI,IAAI,IAAI,CAACvG,MAAM,CAACuG,IAAI;gBACtC5H,iBAAiB0B,QAAQmG,SAAS,IAAI,IAAI,CAACxG,MAAM,CAACwG,SAAS;gBAC3DC,eAAepG,QAAQoG,aAAa,IAAI,IAAI,CAACzG,MAAM,CAACyG,aAAa;YACnE;YACAC,gBAAgB;gBACd;oBACEC,UAAU;oBACVC,WAAW;gBACb;gBACA;oBACED,UAAU;oBACVC,WAAW;gBACb;gBACA;oBACED,UAAU;oBACVC,WAAW;gBACb;gBACA;oBACED,UAAU;oBACVC,WAAW;gBACb;aACD;QACH;IACF;IAEQpG,iBAAiBN,KAAe,EAAU;QAChD,MAAM2G,WAAmC;YACvC,cAAc;YACd,qBAAqB;YACrB,UAAU;YACV,SAAS;QACX;QACA,OAAOA,QAAQ,CAAC3G,MAAM,IAAIA;IAC5B;IAEQwD,gBAAgBoD,MAAc,EAAwC;QAC5E,OAAQA;YACN,KAAK;gBACH,OAAO;YACT,KAAK;gBACH,OAAO;YACT,KAAK;YACL,KAAK;gBACH,OAAO;YACT;gBACE,OAAO;QACX;IACF;IAEQvB,oBAAoBrF,KAAe,EAAU;QACnD,MAAM6G,eAAuC;YAC3C,cAAc;YACd,qBAAqB;YACrB,UAAU;YACV,SAAS;QACX;QACA,OAAOA,YAAY,CAAC7G,MAAM,IAAI;IAChC;IAEA,MAAcuB,oBAAoBV,QAAkB,EAAiB;QACnE,MAAMiG,YAAY,MAAMjG,SAASqB,IAAI;QACrC,IAAI6E;QAEJ,IAAI;YACFA,YAAY7F,KAAK0D,KAAK,CAACkC;QACzB,EAAE,OAAM;YACNC,YAAY;gBAAEtD,OAAO;oBAAEqC,SAASgB;gBAAU;YAAE;QAC9C;QAEA,MAAMhB,UAAUiB,UAAUtD,KAAK,EAAEqC,WAAW;QAE5C,OAAQjF,SAAS8E,MAAM;YACrB,KAAK;YACL,KAAK;gBACH,MAAM,IAAIxH,oBAAoB2H,SAAS,UAAUiB;YACnD,KAAK;gBACH,MAAM,IAAI7I,eAAe4H,SAAS,UAAUlE,WAAWmF;YACzD;gBACE,MAAM,IAAI9I,iBACR6H,SACA,CAAC,OAAO,EAAEjF,SAAS8E,MAAM,EAAE,EAC3B,UACA9E,SAAS8E,MAAM,EACf9E,SAAS8E,MAAM,IAAI,KACnBoB;QAEN;IACF;AACF"}