{"version":3,"sources":["../../../src/providers/base-provider.ts"],"sourcesContent":["/**\n * Abstract Base Provider for LLM integrations\n * Provides common functionality for all LLM providers\n */\n\nimport { EventEmitter } from 'events';\nimport { ILogger } from '../core/logger.js';\nimport { circuitBreaker, CircuitBreaker } from '../utils/helpers.js';\nimport {\n  ILLMProvider,\n  LLMProvider,\n  LLMProviderConfig,\n  LLMRequest,\n  LLMResponse,\n  LLMStreamEvent,\n  LLMModel,\n  ModelInfo,\n  ProviderCapabilities,\n  HealthCheckResult,\n  ProviderStatus,\n  CostEstimate,\n  UsageStats,\n  UsagePeriod,\n  LLMProviderError,\n  RateLimitError,\n  ProviderUnavailableError,\n} from './types.js';\n\nexport interface BaseProviderOptions {\n  logger: ILogger;\n  config: LLMProviderConfig;\n  cacheTTL?: number;\n  circuitBreakerOptions?: {\n    threshold?: number;\n    timeout?: number;\n    resetTimeout?: number;\n  };\n}\n\nexport abstract class BaseProvider extends EventEmitter implements ILLMProvider {\n  abstract readonly name: LLMProvider;\n  abstract readonly capabilities: ProviderCapabilities;\n  \n  protected logger: ILogger;\n  protected circuitBreaker: CircuitBreaker;\n  protected healthCheckInterval?: NodeJS.Timeout;\n  protected lastHealthCheck?: HealthCheckResult;\n  protected requestCount = 0;\n  protected errorCount = 0;\n  protected totalTokens = 0;\n  protected totalCost = 0;\n  protected requestMetrics: Map<string, any> = new Map();\n  \n  public config: LLMProviderConfig;\n\n  constructor(options: BaseProviderOptions) {\n    super();\n    this.logger = options.logger;\n    this.config = options.config;\n    \n    // Initialize circuit breaker\n    this.circuitBreaker = circuitBreaker(`llm-${this.name}`, {\n      threshold: options.circuitBreakerOptions?.threshold || 5,\n      timeout: options.circuitBreakerOptions?.timeout || 60000,\n      resetTimeout: options.circuitBreakerOptions?.resetTimeout || 300000,\n    });\n    \n    // Start health checks if enabled\n    if (this.config.enableCaching) {\n      this.startHealthChecks();\n    }\n  }\n\n  /**\n   * Initialize the provider\n   */\n  async initialize(): Promise<void> {\n    this.logger.info(`Initializing ${this.name} provider`, {\n      model: this.config.model,\n      temperature: this.config.temperature,\n      maxTokens: this.config.maxTokens,\n    });\n    \n    // Validate configuration\n    this.validateConfig();\n    \n    // Provider-specific initialization\n    await this.doInitialize();\n    \n    // Perform initial health check\n    await this.healthCheck();\n  }\n\n  /**\n   * Provider-specific initialization\n   */\n  protected abstract doInitialize(): Promise<void>;\n\n  /**\n   * Validate provider configuration\n   */\n  protected validateConfig(): void {\n    if (!this.config.model) {\n      throw new Error(`Model is required for ${this.name} provider`);\n    }\n    \n    if (!this.validateModel(this.config.model)) {\n      throw new Error(`Model ${this.config.model} is not supported by ${this.name} provider`);\n    }\n    \n    if (this.config.temperature !== undefined) {\n      if (this.config.temperature < 0 || this.config.temperature > 2) {\n        throw new Error('Temperature must be between 0 and 2');\n      }\n    }\n    \n    if (this.config.maxTokens !== undefined) {\n      const maxAllowed = this.capabilities.maxOutputTokens[this.config.model] || 4096;\n      if (this.config.maxTokens > maxAllowed) {\n        throw new Error(`Max tokens exceeds limit of ${maxAllowed} for model ${this.config.model}`);\n      }\n    }\n  }\n\n  /**\n   * Complete a request\n   */\n  async complete(request: LLMRequest): Promise<LLMResponse> {\n    const startTime = Date.now();\n    \n    try {\n      // Use circuit breaker\n      const response = await this.circuitBreaker.execute(async () => {\n        return await this.doComplete(request);\n      });\n      \n      // Track metrics\n      const latency = Date.now() - startTime;\n      this.trackRequest(request, response, latency);\n      \n      // Emit events\n      this.emit('response', {\n        provider: this.name,\n        model: response.model,\n        latency,\n        tokens: response.usage.totalTokens,\n        cost: response.cost?.totalCost,\n      });\n      \n      return response;\n    } catch (error) {\n      this.errorCount++;\n      \n      // Transform to provider error\n      const providerError = this.transformError(error);\n      \n      // Track error\n      this.emit('error', {\n        provider: this.name,\n        error: providerError,\n        request,\n      });\n      \n      throw providerError;\n    }\n  }\n\n  /**\n   * Provider-specific completion implementation\n   */\n  protected abstract doComplete(request: LLMRequest): Promise<LLMResponse>;\n\n  /**\n   * Stream complete a request\n   */\n  async *streamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent> {\n    const startTime = Date.now();\n    let totalTokens = 0;\n    let totalCost = 0;\n    \n    try {\n      // Check if streaming is supported\n      if (!this.capabilities.supportsStreaming) {\n        throw new LLMProviderError(\n          'Streaming not supported',\n          'STREAMING_NOT_SUPPORTED',\n          this.name,\n          undefined,\n          false\n        );\n      }\n      \n      // Use circuit breaker\n      const stream = await this.circuitBreaker.execute(async () => {\n        return this.doStreamComplete(request);\n      });\n      \n      // Process stream\n      for await (const event of stream) {\n        if (event.usage) {\n          totalTokens = event.usage.totalTokens;\n        }\n        if (event.cost) {\n          totalCost = event.cost.totalCost;\n        }\n        \n        yield event;\n      }\n      \n      // Track metrics\n      const latency = Date.now() - startTime;\n      this.trackStreamRequest(request, totalTokens, totalCost, latency);\n      \n    } catch (error) {\n      this.errorCount++;\n      \n      // Transform to provider error\n      const providerError = this.transformError(error);\n      \n      // Yield error event\n      yield {\n        type: 'error',\n        error: providerError,\n      };\n      \n      throw providerError;\n    }\n  }\n\n  /**\n   * Provider-specific stream completion implementation\n   */\n  protected abstract doStreamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent>;\n\n  /**\n   * List available models\n   */\n  abstract listModels(): Promise<LLMModel[]>;\n\n  /**\n   * Get model information\n   */\n  abstract getModelInfo(model: LLMModel): Promise<ModelInfo>;\n\n  /**\n   * Validate if a model is supported\n   */\n  validateModel(model: LLMModel): boolean {\n    return this.capabilities.supportedModels.includes(model);\n  }\n\n  /**\n   * Perform health check\n   */\n  async healthCheck(): Promise<HealthCheckResult> {\n    const startTime = Date.now();\n    \n    try {\n      // Provider-specific health check\n      const result = await this.doHealthCheck();\n      \n      this.lastHealthCheck = {\n        ...result,\n        latency: Date.now() - startTime,\n        timestamp: new Date(),\n      };\n      \n      this.emit('health_check', this.lastHealthCheck);\n      return this.lastHealthCheck;\n      \n    } catch (error) {\n      this.lastHealthCheck = {\n        healthy: false,\n        error: error instanceof Error ? error.message : 'Unknown error',\n        latency: Date.now() - startTime,\n        timestamp: new Date(),\n      };\n      \n      this.emit('health_check', this.lastHealthCheck);\n      return this.lastHealthCheck;\n    }\n  }\n\n  /**\n   * Provider-specific health check implementation\n   */\n  protected abstract doHealthCheck(): Promise<HealthCheckResult>;\n\n  /**\n   * Get provider status\n   */\n  getStatus(): ProviderStatus {\n    const queueLength = this.requestMetrics.size;\n    const errorRate = this.requestCount > 0 ? this.errorCount / this.requestCount : 0;\n    \n    return {\n      available: this.lastHealthCheck?.healthy ?? false,\n      currentLoad: queueLength / 100, // Normalize to 0-1\n      queueLength,\n      activeRequests: queueLength,\n      rateLimitRemaining: this.getRateLimitRemaining(),\n      rateLimitReset: this.getRateLimitReset(),\n    };\n  }\n\n  /**\n   * Get remaining rate limit (override in provider)\n   */\n  protected getRateLimitRemaining(): number | undefined {\n    return undefined;\n  }\n\n  /**\n   * Get rate limit reset time (override in provider)\n   */\n  protected getRateLimitReset(): Date | undefined {\n    return undefined;\n  }\n\n  /**\n   * Estimate cost for a request\n   */\n  async estimateCost(request: LLMRequest): Promise<CostEstimate> {\n    const model = request.model || this.config.model;\n    const pricing = this.capabilities.pricing?.[model];\n    \n    if (!pricing) {\n      return {\n        estimatedPromptTokens: 0,\n        estimatedCompletionTokens: 0,\n        estimatedTotalTokens: 0,\n        estimatedCost: {\n          prompt: 0,\n          completion: 0,\n          total: 0,\n          currency: 'USD',\n        },\n        confidence: 0,\n      };\n    }\n    \n    // Estimate tokens (simple approximation, providers should override)\n    const promptTokens = this.estimateTokens(JSON.stringify(request.messages));\n    const completionTokens = request.maxTokens || this.config.maxTokens || 1000;\n    \n    const promptCost = (promptTokens / 1000) * pricing.promptCostPer1k;\n    const completionCost = (completionTokens / 1000) * pricing.completionCostPer1k;\n    \n    return {\n      estimatedPromptTokens: promptTokens,\n      estimatedCompletionTokens: completionTokens,\n      estimatedTotalTokens: promptTokens + completionTokens,\n      estimatedCost: {\n        prompt: promptCost,\n        completion: completionCost,\n        total: promptCost + completionCost,\n        currency: pricing.currency,\n      },\n      confidence: 0.7, // 70% confidence in estimation\n    };\n  }\n\n  /**\n   * Simple token estimation (4 chars = 1 token approximation)\n   */\n  protected estimateTokens(text: string): number {\n    return Math.ceil(text.length / 4);\n  }\n\n  /**\n   * Get usage statistics\n   */\n  async getUsage(period: UsagePeriod = 'day'): Promise<UsageStats> {\n    const now = new Date();\n    const start = this.getStartDate(now, period);\n    \n    // In a real implementation, this would query a database\n    // For now, return current session stats\n    return {\n      period: { start, end: now },\n      requests: this.requestCount,\n      tokens: {\n        prompt: Math.floor(this.totalTokens * 0.7), // Estimate\n        completion: Math.floor(this.totalTokens * 0.3),\n        total: this.totalTokens,\n      },\n      cost: {\n        prompt: this.totalCost * 0.7,\n        completion: this.totalCost * 0.3,\n        total: this.totalCost,\n        currency: 'USD',\n      },\n      errors: this.errorCount,\n      averageLatency: this.calculateAverageLatency(),\n      modelBreakdown: {}, // Would need to track per model\n    };\n  }\n\n  /**\n   * Get start date for period\n   */\n  private getStartDate(end: Date, period: UsagePeriod): Date {\n    const start = new Date(end);\n    switch (period) {\n      case 'hour':\n        start.setHours(start.getHours() - 1);\n        break;\n      case 'day':\n        start.setDate(start.getDate() - 1);\n        break;\n      case 'week':\n        start.setDate(start.getDate() - 7);\n        break;\n      case 'month':\n        start.setMonth(start.getMonth() - 1);\n        break;\n      case 'all':\n        start.setFullYear(2020); // Arbitrary old date\n        break;\n    }\n    return start;\n  }\n\n  /**\n   * Calculate average latency\n   */\n  private calculateAverageLatency(): number {\n    if (this.requestMetrics.size === 0) return 0;\n    \n    let totalLatency = 0;\n    let count = 0;\n    \n    this.requestMetrics.forEach((metrics) => {\n      if (metrics.latency) {\n        totalLatency += metrics.latency;\n        count++;\n      }\n    });\n    \n    return count > 0 ? totalLatency / count : 0;\n  }\n\n  /**\n   * Track successful request\n   */\n  protected trackRequest(request: LLMRequest, response: LLMResponse, latency: number): void {\n    this.requestCount++;\n    this.totalTokens += response.usage.totalTokens;\n    \n    if (response.cost) {\n      this.totalCost += response.cost.totalCost;\n    }\n    \n    // Store metrics (in memory for now)\n    const requestId = response.id;\n    this.requestMetrics.set(requestId, {\n      timestamp: new Date(),\n      model: response.model,\n      tokens: response.usage.totalTokens,\n      cost: response.cost?.totalCost,\n      latency,\n    });\n    \n    // Clean up old metrics (keep last 1000)\n    if (this.requestMetrics.size > 1000) {\n      const oldestKey = this.requestMetrics.keys().next().value;\n      this.requestMetrics.delete(oldestKey);\n    }\n  }\n\n  /**\n   * Track streaming request\n   */\n  protected trackStreamRequest(\n    request: LLMRequest,\n    totalTokens: number,\n    totalCost: number,\n    latency: number\n  ): void {\n    this.requestCount++;\n    this.totalTokens += totalTokens;\n    this.totalCost += totalCost;\n    \n    // Store metrics\n    const requestId = `stream-${Date.now()}`;\n    this.requestMetrics.set(requestId, {\n      timestamp: new Date(),\n      model: request.model || this.config.model,\n      tokens: totalTokens,\n      cost: totalCost,\n      latency,\n      stream: true,\n    });\n  }\n\n  /**\n   * Transform errors to provider errors\n   */\n  protected transformError(error: unknown): LLMProviderError {\n    if (error instanceof LLMProviderError) {\n      return error;\n    }\n    \n    if (error instanceof Error) {\n      // Check for common error patterns\n      if (error.message.includes('rate limit')) {\n        return new RateLimitError(error.message, this.name);\n      }\n      \n      if (error.message.includes('timeout') || error.message.includes('ETIMEDOUT')) {\n        return new LLMProviderError(\n          'Request timed out',\n          'TIMEOUT',\n          this.name,\n          undefined,\n          true\n        );\n      }\n      \n      if (error.message.includes('ECONNREFUSED') || error.message.includes('fetch failed')) {\n        return new ProviderUnavailableError(this.name, { originalError: error.message });\n      }\n    }\n    \n    return new LLMProviderError(\n      error instanceof Error ? error.message : String(error),\n      'UNKNOWN',\n      this.name,\n      undefined,\n      true\n    );\n  }\n\n  /**\n   * Start periodic health checks\n   */\n  protected startHealthChecks(): void {\n    const interval = this.config.cacheTimeout || 300000; // 5 minutes default\n    \n    this.healthCheckInterval = setInterval(() => {\n      this.healthCheck().catch((error) => {\n        this.logger.error(`Health check failed for ${this.name}`, error);\n      });\n    }, interval);\n  }\n\n  /**\n   * Clean up resources\n   */\n  destroy(): void {\n    if (this.healthCheckInterval) {\n      clearInterval(this.healthCheckInterval);\n    }\n    \n    this.requestMetrics.clear();\n    this.removeAllListeners();\n    \n    this.logger.info(`${this.name} provider destroyed`);\n  }\n}"],"names":["EventEmitter","circuitBreaker","LLMProviderError","RateLimitError","ProviderUnavailableError","BaseProvider","logger","healthCheckInterval","lastHealthCheck","requestCount","errorCount","totalTokens","totalCost","requestMetrics","Map","config","options","name","threshold","circuitBreakerOptions","timeout","resetTimeout","enableCaching","startHealthChecks","initialize","info","model","temperature","maxTokens","validateConfig","doInitialize","healthCheck","Error","validateModel","undefined","maxAllowed","capabilities","maxOutputTokens","complete","request","startTime","Date","now","response","execute","doComplete","latency","trackRequest","emit","provider","tokens","usage","cost","error","providerError","transformError","streamComplete","supportsStreaming","stream","doStreamComplete","event","trackStreamRequest","type","supportedModels","includes","result","doHealthCheck","timestamp","healthy","message","getStatus","queueLength","size","errorRate","available","currentLoad","activeRequests","rateLimitRemaining","getRateLimitRemaining","rateLimitReset","getRateLimitReset","estimateCost","pricing","estimatedPromptTokens","estimatedCompletionTokens","estimatedTotalTokens","estimatedCost","prompt","completion","total","currency","confidence","promptTokens","estimateTokens","JSON","stringify","messages","completionTokens","promptCost","promptCostPer1k","completionCost","completionCostPer1k","text","Math","ceil","length","getUsage","period","start","getStartDate","end","requests","floor","errors","averageLatency","calculateAverageLatency","modelBreakdown","setHours","getHours","setDate","getDate","setMonth","getMonth","setFullYear","totalLatency","count","forEach","metrics","requestId","id","set","oldestKey","keys","next","value","delete","originalError","String","interval","cacheTimeout","setInterval","catch","destroy","clearInterval","clear","removeAllListeners"],"mappings":"AAKA,SAASA,YAAY,QAAQ,SAAS;AAEtC,SAASC,cAAc,QAAwB,sBAAsB;AACrE,SAeEC,gBAAgB,EAChBC,cAAc,EACdC,wBAAwB,QACnB,aAAa;AAapB,OAAO,MAAeC,qBAAqBL;IAI/BM,OAAgB;IAChBL,eAA+B;IAC/BM,oBAAqC;IACrCC,gBAAoC;IACpCC,eAAe,EAAE;IACjBC,aAAa,EAAE;IACfC,cAAc,EAAE;IAChBC,YAAY,EAAE;IACdC,iBAAmC,IAAIC,MAAM;IAEhDC,OAA0B;IAEjC,YAAYC,OAA4B,CAAE;QACxC,KAAK;QACL,IAAI,CAACV,MAAM,GAAGU,QAAQV,MAAM;QAC5B,IAAI,CAACS,MAAM,GAAGC,QAAQD,MAAM;QAG5B,IAAI,CAACd,cAAc,GAAGA,eAAe,CAAC,IAAI,EAAE,IAAI,CAACgB,IAAI,EAAE,EAAE;YACvDC,WAAWF,QAAQG,qBAAqB,EAAED,aAAa;YACvDE,SAASJ,QAAQG,qBAAqB,EAAEC,WAAW;YACnDC,cAAcL,QAAQG,qBAAqB,EAAEE,gBAAgB;QAC/D;QAGA,IAAI,IAAI,CAACN,MAAM,CAACO,aAAa,EAAE;YAC7B,IAAI,CAACC,iBAAiB;QACxB;IACF;IAKA,MAAMC,aAA4B;QAChC,IAAI,CAAClB,MAAM,CAACmB,IAAI,CAAC,CAAC,aAAa,EAAE,IAAI,CAACR,IAAI,CAAC,SAAS,CAAC,EAAE;YACrDS,OAAO,IAAI,CAACX,MAAM,CAACW,KAAK;YACxBC,aAAa,IAAI,CAACZ,MAAM,CAACY,WAAW;YACpCC,WAAW,IAAI,CAACb,MAAM,CAACa,SAAS;QAClC;QAGA,IAAI,CAACC,cAAc;QAGnB,MAAM,IAAI,CAACC,YAAY;QAGvB,MAAM,IAAI,CAACC,WAAW;IACxB;IAUUF,iBAAuB;QAC/B,IAAI,CAAC,IAAI,CAACd,MAAM,CAACW,KAAK,EAAE;YACtB,MAAM,IAAIM,MAAM,CAAC,sBAAsB,EAAE,IAAI,CAACf,IAAI,CAAC,SAAS,CAAC;QAC/D;QAEA,IAAI,CAAC,IAAI,CAACgB,aAAa,CAAC,IAAI,CAAClB,MAAM,CAACW,KAAK,GAAG;YAC1C,MAAM,IAAIM,MAAM,CAAC,MAAM,EAAE,IAAI,CAACjB,MAAM,CAACW,KAAK,CAAC,qBAAqB,EAAE,IAAI,CAACT,IAAI,CAAC,SAAS,CAAC;QACxF;QAEA,IAAI,IAAI,CAACF,MAAM,CAACY,WAAW,KAAKO,WAAW;YACzC,IAAI,IAAI,CAACnB,MAAM,CAACY,WAAW,GAAG,KAAK,IAAI,CAACZ,MAAM,CAACY,WAAW,GAAG,GAAG;gBAC9D,MAAM,IAAIK,MAAM;YAClB;QACF;QAEA,IAAI,IAAI,CAACjB,MAAM,CAACa,SAAS,KAAKM,WAAW;YACvC,MAAMC,aAAa,IAAI,CAACC,YAAY,CAACC,eAAe,CAAC,IAAI,CAACtB,MAAM,CAACW,KAAK,CAAC,IAAI;YAC3E,IAAI,IAAI,CAACX,MAAM,CAACa,SAAS,GAAGO,YAAY;gBACtC,MAAM,IAAIH,MAAM,CAAC,4BAA4B,EAAEG,WAAW,WAAW,EAAE,IAAI,CAACpB,MAAM,CAACW,KAAK,EAAE;YAC5F;QACF;IACF;IAKA,MAAMY,SAASC,OAAmB,EAAwB;QACxD,MAAMC,YAAYC,KAAKC,GAAG;QAE1B,IAAI;YAEF,MAAMC,WAAW,MAAM,IAAI,CAAC1C,cAAc,CAAC2C,OAAO,CAAC;gBACjD,OAAO,MAAM,IAAI,CAACC,UAAU,CAACN;YAC/B;YAGA,MAAMO,UAAUL,KAAKC,GAAG,KAAKF;YAC7B,IAAI,CAACO,YAAY,CAACR,SAASI,UAAUG;YAGrC,IAAI,CAACE,IAAI,CAAC,YAAY;gBACpBC,UAAU,IAAI,CAAChC,IAAI;gBACnBS,OAAOiB,SAASjB,KAAK;gBACrBoB;gBACAI,QAAQP,SAASQ,KAAK,CAACxC,WAAW;gBAClCyC,MAAMT,SAASS,IAAI,EAAExC;YACvB;YAEA,OAAO+B;QACT,EAAE,OAAOU,OAAO;YACd,IAAI,CAAC3C,UAAU;YAGf,MAAM4C,gBAAgB,IAAI,CAACC,cAAc,CAACF;YAG1C,IAAI,CAACL,IAAI,CAAC,SAAS;gBACjBC,UAAU,IAAI,CAAChC,IAAI;gBACnBoC,OAAOC;gBACPf;YACF;YAEA,MAAMe;QACR;IACF;IAUA,OAAOE,eAAejB,OAAmB,EAAiC;QACxE,MAAMC,YAAYC,KAAKC,GAAG;QAC1B,IAAI/B,cAAc;QAClB,IAAIC,YAAY;QAEhB,IAAI;YAEF,IAAI,CAAC,IAAI,CAACwB,YAAY,CAACqB,iBAAiB,EAAE;gBACxC,MAAM,IAAIvD,iBACR,2BACA,2BACA,IAAI,CAACe,IAAI,EACTiB,WACA;YAEJ;YAGA,MAAMwB,SAAS,MAAM,IAAI,CAACzD,cAAc,CAAC2C,OAAO,CAAC;gBAC/C,OAAO,IAAI,CAACe,gBAAgB,CAACpB;YAC/B;YAGA,WAAW,MAAMqB,SAASF,OAAQ;gBAChC,IAAIE,MAAMT,KAAK,EAAE;oBACfxC,cAAciD,MAAMT,KAAK,CAACxC,WAAW;gBACvC;gBACA,IAAIiD,MAAMR,IAAI,EAAE;oBACdxC,YAAYgD,MAAMR,IAAI,CAACxC,SAAS;gBAClC;gBAEA,MAAMgD;YACR;YAGA,MAAMd,UAAUL,KAAKC,GAAG,KAAKF;YAC7B,IAAI,CAACqB,kBAAkB,CAACtB,SAAS5B,aAAaC,WAAWkC;QAE3D,EAAE,OAAOO,OAAO;YACd,IAAI,CAAC3C,UAAU;YAGf,MAAM4C,gBAAgB,IAAI,CAACC,cAAc,CAACF;YAG1C,MAAM;gBACJS,MAAM;gBACNT,OAAOC;YACT;YAEA,MAAMA;QACR;IACF;IAoBArB,cAAcP,KAAe,EAAW;QACtC,OAAO,IAAI,CAACU,YAAY,CAAC2B,eAAe,CAACC,QAAQ,CAACtC;IACpD;IAKA,MAAMK,cAA0C;QAC9C,MAAMS,YAAYC,KAAKC,GAAG;QAE1B,IAAI;YAEF,MAAMuB,SAAS,MAAM,IAAI,CAACC,aAAa;YAEvC,IAAI,CAAC1D,eAAe,GAAG;gBACrB,GAAGyD,MAAM;gBACTnB,SAASL,KAAKC,GAAG,KAAKF;gBACtB2B,WAAW,IAAI1B;YACjB;YAEA,IAAI,CAACO,IAAI,CAAC,gBAAgB,IAAI,CAACxC,eAAe;YAC9C,OAAO,IAAI,CAACA,eAAe;QAE7B,EAAE,OAAO6C,OAAO;YACd,IAAI,CAAC7C,eAAe,GAAG;gBACrB4D,SAAS;gBACTf,OAAOA,iBAAiBrB,QAAQqB,MAAMgB,OAAO,GAAG;gBAChDvB,SAASL,KAAKC,GAAG,KAAKF;gBACtB2B,WAAW,IAAI1B;YACjB;YAEA,IAAI,CAACO,IAAI,CAAC,gBAAgB,IAAI,CAACxC,eAAe;YAC9C,OAAO,IAAI,CAACA,eAAe;QAC7B;IACF;IAUA8D,YAA4B;QAC1B,MAAMC,cAAc,IAAI,CAAC1D,cAAc,CAAC2D,IAAI;QAC5C,MAAMC,YAAY,IAAI,CAAChE,YAAY,GAAG,IAAI,IAAI,CAACC,UAAU,GAAG,IAAI,CAACD,YAAY,GAAG;QAEhF,OAAO;YACLiE,WAAW,IAAI,CAAClE,eAAe,EAAE4D,WAAW;YAC5CO,aAAaJ,cAAc;YAC3BA;YACAK,gBAAgBL;YAChBM,oBAAoB,IAAI,CAACC,qBAAqB;YAC9CC,gBAAgB,IAAI,CAACC,iBAAiB;QACxC;IACF;IAKUF,wBAA4C;QACpD,OAAO5C;IACT;IAKU8C,oBAAsC;QAC9C,OAAO9C;IACT;IAKA,MAAM+C,aAAa1C,OAAmB,EAAyB;QAC7D,MAAMb,QAAQa,QAAQb,KAAK,IAAI,IAAI,CAACX,MAAM,CAACW,KAAK;QAChD,MAAMwD,UAAU,IAAI,CAAC9C,YAAY,CAAC8C,OAAO,EAAE,CAACxD,MAAM;QAElD,IAAI,CAACwD,SAAS;YACZ,OAAO;gBACLC,uBAAuB;gBACvBC,2BAA2B;gBAC3BC,sBAAsB;gBACtBC,eAAe;oBACbC,QAAQ;oBACRC,YAAY;oBACZC,OAAO;oBACPC,UAAU;gBACZ;gBACAC,YAAY;YACd;QACF;QAGA,MAAMC,eAAe,IAAI,CAACC,cAAc,CAACC,KAAKC,SAAS,CAACxD,QAAQyD,QAAQ;QACxE,MAAMC,mBAAmB1D,QAAQX,SAAS,IAAI,IAAI,CAACb,MAAM,CAACa,SAAS,IAAI;QAEvE,MAAMsE,aAAa,AAACN,eAAe,OAAQV,QAAQiB,eAAe;QAClE,MAAMC,iBAAiB,AAACH,mBAAmB,OAAQf,QAAQmB,mBAAmB;QAE9E,OAAO;YACLlB,uBAAuBS;YACvBR,2BAA2Ba;YAC3BZ,sBAAsBO,eAAeK;YACrCX,eAAe;gBACbC,QAAQW;gBACRV,YAAYY;gBACZX,OAAOS,aAAaE;gBACpBV,UAAUR,QAAQQ,QAAQ;YAC5B;YACAC,YAAY;QACd;IACF;IAKUE,eAAeS,IAAY,EAAU;QAC7C,OAAOC,KAAKC,IAAI,CAACF,KAAKG,MAAM,GAAG;IACjC;IAKA,MAAMC,SAASC,SAAsB,KAAK,EAAuB;QAC/D,MAAMjE,MAAM,IAAID;QAChB,MAAMmE,QAAQ,IAAI,CAACC,YAAY,CAACnE,KAAKiE;QAIrC,OAAO;YACLA,QAAQ;gBAAEC;gBAAOE,KAAKpE;YAAI;YAC1BqE,UAAU,IAAI,CAACtG,YAAY;YAC3ByC,QAAQ;gBACNqC,QAAQgB,KAAKS,KAAK,CAAC,IAAI,CAACrG,WAAW,GAAG;gBACtC6E,YAAYe,KAAKS,KAAK,CAAC,IAAI,CAACrG,WAAW,GAAG;gBAC1C8E,OAAO,IAAI,CAAC9E,WAAW;YACzB;YACAyC,MAAM;gBACJmC,QAAQ,IAAI,CAAC3E,SAAS,GAAG;gBACzB4E,YAAY,IAAI,CAAC5E,SAAS,GAAG;gBAC7B6E,OAAO,IAAI,CAAC7E,SAAS;gBACrB8E,UAAU;YACZ;YACAuB,QAAQ,IAAI,CAACvG,UAAU;YACvBwG,gBAAgB,IAAI,CAACC,uBAAuB;YAC5CC,gBAAgB,CAAC;QACnB;IACF;IAKQP,aAAaC,GAAS,EAAEH,MAAmB,EAAQ;QACzD,MAAMC,QAAQ,IAAInE,KAAKqE;QACvB,OAAQH;YACN,KAAK;gBACHC,MAAMS,QAAQ,CAACT,MAAMU,QAAQ,KAAK;gBAClC;YACF,KAAK;gBACHV,MAAMW,OAAO,CAACX,MAAMY,OAAO,KAAK;gBAChC;YACF,KAAK;gBACHZ,MAAMW,OAAO,CAACX,MAAMY,OAAO,KAAK;gBAChC;YACF,KAAK;gBACHZ,MAAMa,QAAQ,CAACb,MAAMc,QAAQ,KAAK;gBAClC;YACF,KAAK;gBACHd,MAAMe,WAAW,CAAC;gBAClB;QACJ;QACA,OAAOf;IACT;IAKQO,0BAAkC;QACxC,IAAI,IAAI,CAACtG,cAAc,CAAC2D,IAAI,KAAK,GAAG,OAAO;QAE3C,IAAIoD,eAAe;QACnB,IAAIC,QAAQ;QAEZ,IAAI,CAAChH,cAAc,CAACiH,OAAO,CAAC,CAACC;YAC3B,IAAIA,QAAQjF,OAAO,EAAE;gBACnB8E,gBAAgBG,QAAQjF,OAAO;gBAC/B+E;YACF;QACF;QAEA,OAAOA,QAAQ,IAAID,eAAeC,QAAQ;IAC5C;IAKU9E,aAAaR,OAAmB,EAAEI,QAAqB,EAAEG,OAAe,EAAQ;QACxF,IAAI,CAACrC,YAAY;QACjB,IAAI,CAACE,WAAW,IAAIgC,SAASQ,KAAK,CAACxC,WAAW;QAE9C,IAAIgC,SAASS,IAAI,EAAE;YACjB,IAAI,CAACxC,SAAS,IAAI+B,SAASS,IAAI,CAACxC,SAAS;QAC3C;QAGA,MAAMoH,YAAYrF,SAASsF,EAAE;QAC7B,IAAI,CAACpH,cAAc,CAACqH,GAAG,CAACF,WAAW;YACjC7D,WAAW,IAAI1B;YACff,OAAOiB,SAASjB,KAAK;YACrBwB,QAAQP,SAASQ,KAAK,CAACxC,WAAW;YAClCyC,MAAMT,SAASS,IAAI,EAAExC;YACrBkC;QACF;QAGA,IAAI,IAAI,CAACjC,cAAc,CAAC2D,IAAI,GAAG,MAAM;YACnC,MAAM2D,YAAY,IAAI,CAACtH,cAAc,CAACuH,IAAI,GAAGC,IAAI,GAAGC,KAAK;YACzD,IAAI,CAACzH,cAAc,CAAC0H,MAAM,CAACJ;QAC7B;IACF;IAKUtE,mBACRtB,OAAmB,EACnB5B,WAAmB,EACnBC,SAAiB,EACjBkC,OAAe,EACT;QACN,IAAI,CAACrC,YAAY;QACjB,IAAI,CAACE,WAAW,IAAIA;QACpB,IAAI,CAACC,SAAS,IAAIA;QAGlB,MAAMoH,YAAY,CAAC,OAAO,EAAEvF,KAAKC,GAAG,IAAI;QACxC,IAAI,CAAC7B,cAAc,CAACqH,GAAG,CAACF,WAAW;YACjC7D,WAAW,IAAI1B;YACff,OAAOa,QAAQb,KAAK,IAAI,IAAI,CAACX,MAAM,CAACW,KAAK;YACzCwB,QAAQvC;YACRyC,MAAMxC;YACNkC;YACAY,QAAQ;QACV;IACF;IAKUH,eAAeF,KAAc,EAAoB;QACzD,IAAIA,iBAAiBnD,kBAAkB;YACrC,OAAOmD;QACT;QAEA,IAAIA,iBAAiBrB,OAAO;YAE1B,IAAIqB,MAAMgB,OAAO,CAACL,QAAQ,CAAC,eAAe;gBACxC,OAAO,IAAI7D,eAAekD,MAAMgB,OAAO,EAAE,IAAI,CAACpD,IAAI;YACpD;YAEA,IAAIoC,MAAMgB,OAAO,CAACL,QAAQ,CAAC,cAAcX,MAAMgB,OAAO,CAACL,QAAQ,CAAC,cAAc;gBAC5E,OAAO,IAAI9D,iBACT,qBACA,WACA,IAAI,CAACe,IAAI,EACTiB,WACA;YAEJ;YAEA,IAAImB,MAAMgB,OAAO,CAACL,QAAQ,CAAC,mBAAmBX,MAAMgB,OAAO,CAACL,QAAQ,CAAC,iBAAiB;gBACpF,OAAO,IAAI5D,yBAAyB,IAAI,CAACa,IAAI,EAAE;oBAAEuH,eAAenF,MAAMgB,OAAO;gBAAC;YAChF;QACF;QAEA,OAAO,IAAInE,iBACTmD,iBAAiBrB,QAAQqB,MAAMgB,OAAO,GAAGoE,OAAOpF,QAChD,WACA,IAAI,CAACpC,IAAI,EACTiB,WACA;IAEJ;IAKUX,oBAA0B;QAClC,MAAMmH,WAAW,IAAI,CAAC3H,MAAM,CAAC4H,YAAY,IAAI;QAE7C,IAAI,CAACpI,mBAAmB,GAAGqI,YAAY;YACrC,IAAI,CAAC7G,WAAW,GAAG8G,KAAK,CAAC,CAACxF;gBACxB,IAAI,CAAC/C,MAAM,CAAC+C,KAAK,CAAC,CAAC,wBAAwB,EAAE,IAAI,CAACpC,IAAI,EAAE,EAAEoC;YAC5D;QACF,GAAGqF;IACL;IAKAI,UAAgB;QACd,IAAI,IAAI,CAACvI,mBAAmB,EAAE;YAC5BwI,cAAc,IAAI,CAACxI,mBAAmB;QACxC;QAEA,IAAI,CAACM,cAAc,CAACmI,KAAK;QACzB,IAAI,CAACC,kBAAkB;QAEvB,IAAI,CAAC3I,MAAM,CAACmB,IAAI,CAAC,GAAG,IAAI,CAACR,IAAI,CAAC,mBAAmB,CAAC;IACpD;AACF"}