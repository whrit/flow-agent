{"version":3,"sources":["../../../src/providers/types.ts"],"sourcesContent":["/**\n * Multi-LLM Provider Types and Interfaces\n * Unified type system for all LLM providers\n */\n\nimport { EventEmitter } from 'events';\n\n// ===== PROVIDER TYPES =====\n\nexport type LLMProvider = \n  | 'openai'\n  | 'anthropic'\n  | 'google'\n  | 'cohere'\n  | 'ollama'\n  | 'llama-cpp'\n  | 'custom';\n\nexport type LLMModel =\n  // OpenAI Models\n  | 'gpt-4-turbo-preview'\n  | 'gpt-4'\n  | 'gpt-4-32k'\n  | 'gpt-3.5-turbo'\n  | 'gpt-3.5-turbo-16k'\n  // Anthropic Models\n  | 'claude-3-opus-20240229'\n  | 'claude-3-sonnet-20240229'\n  | 'claude-3-haiku-20240307'\n  | 'claude-2.1'\n  | 'claude-2.0'\n  | 'claude-instant-1.2'\n  // Google Models\n  | 'gemini-pro'\n  | 'gemini-pro-vision'\n  | 'palm-2'\n  | 'bison'\n  // Cohere Models\n  | 'command'\n  | 'command-light'\n  | 'command-nightly'\n  | 'generate-xlarge'\n  | 'generate-medium'\n  // Local Models\n  | 'llama-2-7b'\n  | 'llama-2-13b'\n  | 'llama-2-70b'\n  | 'mistral-7b'\n  | 'mixtral-8x7b'\n  | 'custom-model';\n\n// ===== BASE INTERFACES =====\n\nexport interface LLMProviderConfig {\n  provider: LLMProvider;\n  apiKey?: string;\n  apiUrl?: string;\n  model: LLMModel;\n  \n  // Common parameters\n  temperature?: number;\n  maxTokens?: number;\n  topP?: number;\n  topK?: number;\n  frequencyPenalty?: number;\n  presencePenalty?: number;\n  stopSequences?: string[];\n  \n  // Provider-specific settings\n  providerOptions?: Record<string, any>;\n  \n  // Performance settings\n  timeout?: number;\n  retryAttempts?: number;\n  retryDelay?: number;\n  \n  // Advanced features\n  enableStreaming?: boolean;\n  enableCaching?: boolean;\n  cacheTimeout?: number;\n  \n  // Cost optimization\n  enableCostOptimization?: boolean;\n  maxCostPerRequest?: number;\n  fallbackModels?: LLMModel[];\n}\n\nexport interface LLMMessage {\n  role: 'system' | 'user' | 'assistant' | 'function';\n  content: string;\n  name?: string; // For function messages\n  functionCall?: {\n    name: string;\n    arguments: string;\n  };\n}\n\nexport interface LLMRequest {\n  messages: LLMMessage[];\n  model?: LLMModel;\n  temperature?: number;\n  maxTokens?: number;\n  topP?: number;\n  topK?: number;\n  frequencyPenalty?: number;\n  presencePenalty?: number;\n  stopSequences?: string[];\n  stream?: boolean;\n  \n  // Function calling\n  functions?: LLMFunction[];\n  functionCall?: 'auto' | 'none' | { name: string };\n  \n  // Provider-specific options\n  providerOptions?: Record<string, any>;\n  \n  // Cost optimization\n  costConstraints?: {\n    maxCost?: number;\n    preferredModels?: LLMModel[];\n  };\n}\n\nexport interface LLMFunction {\n  name: string;\n  description: string;\n  parameters: {\n    type: 'object';\n    properties: Record<string, any>;\n    required?: string[];\n  };\n}\n\nexport interface LLMResponse {\n  id: string;\n  model: LLMModel;\n  provider: LLMProvider;\n  \n  // Content\n  content: string;\n  functionCall?: {\n    name: string;\n    arguments: string;\n  };\n  \n  // Metadata\n  usage: {\n    promptTokens: number;\n    completionTokens: number;\n    totalTokens: number;\n  };\n  \n  // Cost tracking\n  cost?: {\n    promptCost: number;\n    completionCost: number;\n    totalCost: number;\n    currency: string;\n  };\n  \n  // Performance metrics\n  latency?: number;\n  \n  // Additional info\n  finishReason?: 'stop' | 'length' | 'function_call' | 'content_filter';\n  metadata?: Record<string, any>;\n}\n\nexport interface LLMStreamEvent {\n  type: 'content' | 'function_call' | 'error' | 'done';\n  delta?: {\n    content?: string;\n    functionCall?: {\n      name?: string;\n      arguments?: string;\n    };\n  };\n  error?: Error;\n  usage?: LLMResponse['usage'];\n  cost?: LLMResponse['cost'];\n}\n\n// ===== PROVIDER CAPABILITIES =====\n\nexport interface ProviderCapabilities {\n  // Model features\n  supportedModels: LLMModel[];\n  maxContextLength: Record<LLMModel, number>;\n  maxOutputTokens: Record<LLMModel, number>;\n  \n  // Feature support\n  supportsStreaming: boolean;\n  supportsFunctionCalling: boolean;\n  supportsSystemMessages: boolean;\n  supportsVision: boolean;\n  supportsAudio: boolean;\n  supportsTools: boolean;\n  \n  // Advanced features\n  supportsFineTuning: boolean;\n  supportsEmbeddings: boolean;\n  supportsLogprobs: boolean;\n  supportsBatching: boolean;\n  \n  // Constraints\n  rateLimit?: {\n    requestsPerMinute: number;\n    tokensPerMinute: number;\n    concurrentRequests: number;\n  };\n  \n  // Cost information\n  pricing?: {\n    [model: string]: {\n      promptCostPer1k: number;\n      completionCostPer1k: number;\n      currency: string;\n    };\n  };\n}\n\n// ===== ERROR HANDLING =====\n\nexport class LLMProviderError extends Error {\n  constructor(\n    message: string,\n    public code: string,\n    public provider: LLMProvider,\n    public statusCode?: number,\n    public retryable: boolean = true,\n    public details?: any\n  ) {\n    super(message);\n    this.name = 'LLMProviderError';\n  }\n}\n\nexport class RateLimitError extends LLMProviderError {\n  constructor(\n    message: string,\n    provider: LLMProvider,\n    public retryAfter?: number,\n    details?: any\n  ) {\n    super(message, 'RATE_LIMIT', provider, 429, true, details);\n    this.name = 'RateLimitError';\n  }\n}\n\nexport class AuthenticationError extends LLMProviderError {\n  constructor(message: string, provider: LLMProvider, details?: any) {\n    super(message, 'AUTHENTICATION', provider, 401, false, details);\n    this.name = 'AuthenticationError';\n  }\n}\n\nexport class ModelNotFoundError extends LLMProviderError {\n  constructor(model: string, provider: LLMProvider, details?: any) {\n    super(`Model ${model} not found`, 'MODEL_NOT_FOUND', provider, 404, false, details);\n    this.name = 'ModelNotFoundError';\n  }\n}\n\nexport class ProviderUnavailableError extends LLMProviderError {\n  constructor(provider: LLMProvider, details?: any) {\n    super(`Provider ${provider} is unavailable`, 'PROVIDER_UNAVAILABLE', provider, 503, true, details);\n    this.name = 'ProviderUnavailableError';\n  }\n}\n\n// ===== ABSTRACT PROVIDER INTERFACE =====\n\nexport interface ILLMProvider extends EventEmitter {\n  // Properties\n  readonly name: LLMProvider;\n  readonly capabilities: ProviderCapabilities;\n  config: LLMProviderConfig;\n  \n  // Core methods\n  initialize(): Promise<void>;\n  complete(request: LLMRequest): Promise<LLMResponse>;\n  streamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent>;\n  \n  // Model management\n  listModels(): Promise<LLMModel[]>;\n  getModelInfo(model: LLMModel): Promise<ModelInfo>;\n  validateModel(model: LLMModel): boolean;\n  \n  // Health and status\n  healthCheck(): Promise<HealthCheckResult>;\n  getStatus(): ProviderStatus;\n  \n  // Cost management\n  estimateCost(request: LLMRequest): Promise<CostEstimate>;\n  getUsage(period?: UsagePeriod): Promise<UsageStats>;\n  \n  // Cleanup\n  destroy(): void;\n}\n\nexport interface ModelInfo {\n  model: LLMModel;\n  name: string;\n  description: string;\n  contextLength: number;\n  maxOutputTokens: number;\n  supportedFeatures: string[];\n  pricing?: {\n    promptCostPer1k: number;\n    completionCostPer1k: number;\n    currency: string;\n  };\n  deprecated?: boolean;\n  deprecationDate?: Date;\n  recommendedReplacement?: LLMModel;\n}\n\nexport interface HealthCheckResult {\n  healthy: boolean;\n  latency?: number;\n  error?: string;\n  timestamp: Date;\n  details?: Record<string, any>;\n}\n\nexport interface ProviderStatus {\n  available: boolean;\n  currentLoad: number;\n  queueLength: number;\n  activeRequests: number;\n  rateLimitRemaining?: number;\n  rateLimitReset?: Date;\n}\n\nexport interface CostEstimate {\n  estimatedPromptTokens: number;\n  estimatedCompletionTokens: number;\n  estimatedTotalTokens: number;\n  estimatedCost: {\n    prompt: number;\n    completion: number;\n    total: number;\n    currency: string;\n  };\n  confidence: number; // 0-1\n}\n\nexport interface UsageStats {\n  period: {\n    start: Date;\n    end: Date;\n  };\n  requests: number;\n  tokens: {\n    prompt: number;\n    completion: number;\n    total: number;\n  };\n  cost: {\n    prompt: number;\n    completion: number;\n    total: number;\n    currency: string;\n  };\n  errors: number;\n  averageLatency: number;\n  modelBreakdown: Record<LLMModel, {\n    requests: number;\n    tokens: number;\n    cost: number;\n  }>;\n}\n\nexport type UsagePeriod = 'hour' | 'day' | 'week' | 'month' | 'all';\n\n// ===== FALLBACK AND RETRY STRATEGIES =====\n\nexport interface FallbackStrategy {\n  name: string;\n  enabled: boolean;\n  rules: FallbackRule[];\n  maxAttempts: number;\n}\n\nexport interface FallbackRule {\n  condition: 'error' | 'rate_limit' | 'timeout' | 'cost' | 'unavailable';\n  errorCodes?: string[];\n  fallbackProviders: LLMProvider[];\n  fallbackModels?: LLMModel[];\n  retryOriginal: boolean;\n  retryDelay?: number;\n}\n\nexport interface RetryStrategy {\n  maxAttempts: number;\n  initialDelay: number;\n  maxDelay: number;\n  backoffMultiplier: number;\n  jitter: boolean;\n  retryableErrors: string[];\n}\n\n// ===== CACHING INTERFACES =====\n\nexport interface CacheConfig {\n  enabled: boolean;\n  ttl: number; // Time to live in seconds\n  maxSize: number; // Max cache size in MB\n  strategy: 'lru' | 'lfu' | 'ttl';\n  keyGenerator?: (request: LLMRequest) => string;\n}\n\nexport interface CacheEntry {\n  key: string;\n  request: LLMRequest;\n  response: LLMResponse;\n  timestamp: Date;\n  hits: number;\n  size: number;\n}\n\n// ===== RATE LIMITING =====\n\nexport interface RateLimiter {\n  checkLimit(provider: LLMProvider, model?: LLMModel): Promise<boolean>;\n  consumeToken(provider: LLMProvider, tokens: number): Promise<void>;\n  getRemainingTokens(provider: LLMProvider): Promise<number>;\n  getResetTime(provider: LLMProvider): Promise<Date | null>;\n  waitForCapacity(provider: LLMProvider, tokens: number): Promise<void>;\n}\n\n// ===== LOAD BALANCING =====\n\nexport interface LoadBalancer {\n  selectProvider(request: LLMRequest, availableProviders: ILLMProvider[]): Promise<ILLMProvider>;\n  updateProviderMetrics(provider: LLMProvider, metrics: ProviderMetrics): void;\n  rebalance(): Promise<void>;\n}\n\nexport interface ProviderMetrics {\n  provider: LLMProvider;\n  timestamp: Date;\n  latency: number;\n  errorRate: number;\n  successRate: number;\n  load: number;\n  cost: number;\n  availability: number;\n}\n\n// ===== MONITORING AND ANALYTICS =====\n\nexport interface ProviderMonitor {\n  trackRequest(provider: LLMProvider, request: LLMRequest, response: LLMResponse | Error): void;\n  getMetrics(provider?: LLMProvider, period?: UsagePeriod): Promise<ProviderMetrics[]>;\n  getAlerts(): Alert[];\n  setAlertThreshold(metric: string, threshold: number): void;\n}\n\nexport interface Alert {\n  id: string;\n  timestamp: Date;\n  provider: LLMProvider;\n  type: 'error_rate' | 'latency' | 'cost' | 'rate_limit' | 'availability';\n  severity: 'info' | 'warning' | 'error' | 'critical';\n  message: string;\n  value: number;\n  threshold: number;\n}\n\n// ===== COST OPTIMIZATION =====\n\nexport interface CostOptimizer {\n  selectOptimalModel(request: LLMRequest, constraints: CostConstraints): Promise<OptimizationResult>;\n  analyzeCostTrends(period: UsagePeriod): Promise<CostAnalysis>;\n  suggestOptimizations(): Promise<OptimizationSuggestion[]>;\n}\n\nexport interface CostConstraints {\n  maxCostPerRequest?: number;\n  maxCostPerToken?: number;\n  preferredProviders?: LLMProvider[];\n  requiredFeatures?: string[];\n  minQuality?: number; // 0-1\n}\n\nexport interface OptimizationResult {\n  provider: LLMProvider;\n  model: LLMModel;\n  estimatedCost: number;\n  estimatedQuality: number; // 0-1\n  reasoning: string;\n}\n\nexport interface CostAnalysis {\n  period: UsagePeriod;\n  totalCost: number;\n  costByProvider: Record<LLMProvider, number>;\n  costByModel: Record<LLMModel, number>;\n  trends: {\n    dailyAverage: number;\n    weeklyGrowth: number;\n    projection30Days: number;\n  };\n}\n\nexport interface OptimizationSuggestion {\n  type: 'model_switch' | 'provider_switch' | 'parameter_tuning' | 'caching' | 'batching';\n  description: string;\n  estimatedSavings: number;\n  implementation: string;\n  impact: 'low' | 'medium' | 'high';\n}\n\n// ===== TYPE GUARDS =====\n\nexport function isLLMResponse(obj: any): obj is LLMResponse {\n  return obj && typeof obj.id === 'string' && typeof obj.content === 'string';\n}\n\nexport function isLLMStreamEvent(obj: any): obj is LLMStreamEvent {\n  return obj && typeof obj.type === 'string';\n}\n\nexport function isLLMProviderError(error: any): error is LLMProviderError {\n  return error instanceof LLMProviderError;\n}\n\nexport function isRateLimitError(error: any): error is RateLimitError {\n  return error instanceof RateLimitError;\n}"],"names":["LLMProviderError","Error","message","code","provider","statusCode","retryable","details","name","RateLimitError","retryAfter","AuthenticationError","ModelNotFoundError","model","ProviderUnavailableError","isLLMResponse","obj","id","content","isLLMStreamEvent","type","isLLMProviderError","error","isRateLimitError"],"mappings":"AA+NA,OAAO,MAAMA,yBAAyBC;;;;;;IACpC,YACEC,OAAe,EACf,AAAOC,IAAY,EACnB,AAAOC,QAAqB,EAC5B,AAAOC,UAAmB,EAC1B,AAAOC,YAAqB,IAAI,EAChC,AAAOC,OAAa,CACpB;QACA,KAAK,CAACL,eANCC,OAAAA,WACAC,WAAAA,eACAC,aAAAA,iBACAC,YAAAA,gBACAC,UAAAA;QAGP,IAAI,CAACC,IAAI,GAAG;IACd;AACF;AAEA,OAAO,MAAMC,uBAAuBT;;IAClC,YACEE,OAAe,EACfE,QAAqB,EACrB,AAAOM,UAAmB,EAC1BH,OAAa,CACb;QACA,KAAK,CAACL,SAAS,cAAcE,UAAU,KAAK,MAAMG,eAH3CG,aAAAA;QAIP,IAAI,CAACF,IAAI,GAAG;IACd;AACF;AAEA,OAAO,MAAMG,4BAA4BX;IACvC,YAAYE,OAAe,EAAEE,QAAqB,EAAEG,OAAa,CAAE;QACjE,KAAK,CAACL,SAAS,kBAAkBE,UAAU,KAAK,OAAOG;QACvD,IAAI,CAACC,IAAI,GAAG;IACd;AACF;AAEA,OAAO,MAAMI,2BAA2BZ;IACtC,YAAYa,KAAa,EAAET,QAAqB,EAAEG,OAAa,CAAE;QAC/D,KAAK,CAAC,CAAC,MAAM,EAAEM,MAAM,UAAU,CAAC,EAAE,mBAAmBT,UAAU,KAAK,OAAOG;QAC3E,IAAI,CAACC,IAAI,GAAG;IACd;AACF;AAEA,OAAO,MAAMM,iCAAiCd;IAC5C,YAAYI,QAAqB,EAAEG,OAAa,CAAE;QAChD,KAAK,CAAC,CAAC,SAAS,EAAEH,SAAS,eAAe,CAAC,EAAE,wBAAwBA,UAAU,KAAK,MAAMG;QAC1F,IAAI,CAACC,IAAI,GAAG;IACd;AACF;AAwPA,OAAO,SAASO,cAAcC,GAAQ;IACpC,OAAOA,OAAO,OAAOA,IAAIC,EAAE,KAAK,YAAY,OAAOD,IAAIE,OAAO,KAAK;AACrE;AAEA,OAAO,SAASC,iBAAiBH,GAAQ;IACvC,OAAOA,OAAO,OAAOA,IAAII,IAAI,KAAK;AACpC;AAEA,OAAO,SAASC,mBAAmBC,KAAU;IAC3C,OAAOA,iBAAiBtB;AAC1B;AAEA,OAAO,SAASuB,iBAAiBD,KAAU;IACzC,OAAOA,iBAAiBb;AAC1B"}